<!DOCTYPE html>
<html lang="en"><head>
<script src="L8_files/libs/clipboard/clipboard.min.js"></script>
<script src="L8_files/libs/quarto-html/tabby.min.js"></script>
<script src="L8_files/libs/quarto-html/popper.min.js"></script>
<script src="L8_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="L8_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="L8_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="L8_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="L8_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.3.433">

  <title>Lecture 8</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="L8_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="L8_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="L8_files/libs/revealjs/dist/theme/quarto.css">
  <link href="L8_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="L8_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="L8_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="L8_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="L8_files/libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="L8_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Lecture 8</h1>
  <p class="subtitle">HHU summer term 2025 <br> June 18, 2025 <br> Prof.&nbsp;Dr.&nbsp;Jannis Kück</p>

<div class="quarto-title-authors">
</div>

</section>
<section>
<section id="statistical-inference-on-predictive-and-causal-effects-in-modern-nonlinear-regression-models" class="title-slide slide level1 center">
<h1>Statistical Inference on Predictive and Causal Effects in Modern Nonlinear Regression Models</h1>

</section>
<section id="introduction" class="slide level2">
<h2>Introduction</h2>
<ul>
<li><p>Here we discuss double/debiased machine learning (DML) methods for performing inference on average predictive or causal effects in the important classes of models: partially linear regression models, interactive regression models and interactive IV models.</p></li>
<li><p>We also present a general DML method for performing inference on a low-dimensional target parameter in the presence of high dimensional nuisance parameters that are learned using ML methods.</p></li>
<li><p>Two case studies illustrate the approach in the tutorial. In the first, we perform inference on the effect of gun ownership on homicide rates. In the second, we perform inference on the effect of 401(k) eligibility on financial assets.</p></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><p>We recall the predictive effect question: How does the predicted value of outcome, <span class="math display">\[\mathbb{E}[Y \mid D, X],\]</span> change if a regressor value <span class="math inline">\(D\)</span> increases by a unit, while regressor values <span class="math inline">\(X\)</span> remain unchanged?</p></li>
<li><p>This question may have a causal interpretation within any SEM, where conditioning on <span class="math inline">\(X\)</span> is sufficient for identification of the causal effect of <span class="math inline">\(D\)</span> on <span class="math inline">\(Y\)</span>. When this condition holds, the question becomes the causal effect question: How does the predicted value of potential outcome, <span class="math display">\[\mathbb{E}[Y(d) \mid X],\]</span> change if we intervene and change the treatment value <span class="math inline">\(d\)</span> by a unit, conditional on the observed <span class="math inline">\(X\)</span>?</p></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><p>In what follows, we set up double/debiased machine learning (DML) methods for answering these questions with data. These statistical inference methods <strong>do not</strong> distinguish between the two types of questions, so the methods are equally applicable to answering both types.</p></li>
<li><p>The DML method requires a Neyman-orthogonal representation of the target parameters to reduce the spillover of regularization biases inherent in ML methods onto the estimation of the target parameter. The method also makes use of cross-fitting: an efficient form of sample splitting that eliminates biases that may arise from overfitting.</p></li>
</ul>
</section>
<section id="dml-inference-in-the-partially-linear-regression-model-plm" class="slide level2">
<h2>DML Inference in the Partially Linear Regression Model (PLM)</h2>
<ul>
<li>We first answer the questions posed above within the context of the partially linear regression model: <span id="eq-PLM"><span class="math display">\[
\begin{equation}\label{eq:PLM}
Y = \alpha D + g(X) + \epsilon,  \quad  \mathbb{E}[\epsilon \mid D, X]= 0,
\end{equation}
\qquad(1)\]</span></span> where <span class="math inline">\(Y\)</span> is the outcome variable, <span class="math inline">\(D\)</span> is the regressor of interest, and <span class="math inline">\(X\)</span> is a high-dimensional vector of other regressors or features, called “controls.”</li>
<li>The coefficient <span class="math inline">\(\alpha\)</span> is the target parameter of interest. In the following we discuss the estimation of <span class="math inline">\(\alpha\)</span> and how to construct valid confidence intervals.</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><p>The model allows a part of the regression function, <span class="math inline">\(g(X)\)</span>, to be fully nonlinear, which generalizes the approach we have considered before.</p></li>
<li><p>However, the model is still not fully general, because it imposes additivity in <span class="math inline">\(g(X)\)</span> and <span class="math inline">\(D\)</span>. We will consider the fully unrestricted model later, where we analyze the fully interactive regression model (IRM) in the context of a binary treatment <span class="math inline">\(D\)</span>.</p></li>
<li><p>It is worth pointing out though that the partially linear model is not as restrictive as it appears at a first sight since we can consider explicit interactions within the partially linear framework.</p></li>
</ul>
</section>
<section id="simultaneous-inference" class="slide level2">
<h2>Simultaneous Inference</h2>
<ul>
<li><p>Given a raw treatment, <span class="math inline">\(\bar D\)</span>, we can create the technical treatment <span class="math inline">\(D: = \bar D P(Z)\)</span>, where <span class="math inline">\(P(Z)\)</span> is an <span class="math inline">\(L-\)</span>dimensional dictionary of transformations of the control variables <span class="math inline">\(Z\)</span>.</p></li>
<li><p>Then we can consider the model <span class="math display">\[Y = \sum_{l=1}^L \alpha_l D_l+ g(Z)  + \epsilon,\]</span> where <span class="math inline">\(\mathbb{E}[\epsilon \mid Z, D]= 0\)</span>. We can re-write this as <span class="math display">\[Y = \alpha_l D_l + g_l(X_l) + \epsilon, \quad \mathbb{E}[\epsilon \mid D_l, X_l] = 0,\]</span> where <span class="math inline">\(g_l(X_l) := \sum_{k \neq l} \alpha_k D_k + g(Z)\)</span> and <span class="math inline">\(X_l:= ( ( D_k)_{k \neq l}, Z)\)</span>.</p></li>
<li><p>We therefore obtain exactly a model of the partially linear form in <a href="#/dml-inference-in-the-partially-linear-regression-model-plm">Equation&nbsp;1</a>. We can apply DML methods to learn and perform inference on each element of <span class="math inline">\((\alpha_l)_{l=1}^L\)</span> or even carry out joint inference (similarly to what we have done in Lecture 4).</p></li>
</ul>
</section>
<section id="partialling-out" class="slide level2">
<h2>Partialling-Out</h2>
<ul>
<li>In what follows, we will employ the partialling-out <span class="math inline">\(X\)</span> operation of the form that inputs a random variable <span class="math inline">\(V\)</span> and outputs the residualized form: <span class="math display">\[
\tilde V : = V - \mathbb{E}[V \mid X].
\]</span> Applying this operation to <a href="#/dml-inference-in-the-partially-linear-regression-model-plm">Equation&nbsp;1</a> we obtain: <span class="math display">\[
\begin{equation}\label{decompose3}
\tilde Y = \alpha \tilde D+ \epsilon,  \quad \mathbb{E}[\epsilon \tilde D] =0,
\end{equation}
\]</span> where <span class="math inline">\(\tilde Y\)</span> and <span class="math inline">\(\tilde D\)</span> are the residuals left after predicting <span class="math inline">\(Y\)</span> and <span class="math inline">\(D\)</span> using <span class="math inline">\(X\)</span>. Specifically, we have that <span class="math display">\[
\tilde Y := Y - \ell(X) \ \text{and} \ \tilde D := D - m(X),
\]</span> where <span class="math inline">\(\ell(X)\)</span> and <span class="math inline">\(m(X)\)</span> are defined as conditional expectations of <span class="math inline">\(Y\)</span> and <span class="math inline">\(D\)</span> given <span class="math inline">\(X\)</span>: <span class="math display">\[
\ell(X) :=  \mathbb{E}[Y \mid X] \ \text{and} \  m(X) : = \mathbb{E}[D \mid X].
\]</span></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><p>Here we recall that the conditional expectations of <span class="math inline">\(Y\)</span> and <span class="math inline">\(D\)</span> given <span class="math inline">\(X\)</span> are the best predictors of <span class="math inline">\(Y\)</span> and <span class="math inline">\(D\)</span> using <span class="math inline">\(X\)</span>.</p></li>
<li><p>The equation <span class="math inline">\(\mathbb{E}[\epsilon \tilde D]= 0\)</span> above is the Normal Equation for the population regression of <span class="math inline">\(\tilde Y\)</span> on <span class="math inline">\(\tilde D\)</span>.</p></li>
</ul>
</section>
<section id="fwl-partialling-out-for-partially-linear-model" class="slide level2">
<h2>FWL Partialling-Out for Partially Linear Model</h2>
<div id="thm-FWL" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 </strong></span>Suppose that <span class="math inline">\(Y\)</span>, <span class="math inline">\(X\)</span> and <span class="math inline">\(D\)</span> have bounded second moments. Then the population regression coefficient <span class="math inline">\(\alpha\)</span> can be recovered from the population linear regression of <span class="math inline">\(\tilde Y\)</span> on <span class="math inline">\(\tilde D\)</span>: <span class="math display">\[
\alpha :=  \{a:  \mathbb{E}[(\tilde Y - a \tilde D) \tilde D]  = 0 \} :=  \mathbb{E}[\tilde D^2]^{-1} \mathbb{E}[\tilde D \tilde Y],
\]</span> where <span class="math inline">\(\alpha\)</span> is uniquely defined if <span class="math inline">\(D\)</span> cannot be perfectly predicted by <span class="math inline">\(X\)</span>, i.e.&nbsp;if <span class="math inline">\(\mathbb{E} [\tilde D^2] &gt; 0\)</span>.</p>
</div>
<ul>
<li><p>Thus, <span class="math inline">\(\alpha\)</span> can be interpreted as a regression coefficient of <em>residualized</em> <span class="math inline">\(Y\)</span> on <em>residualized</em> <span class="math inline">\(D\)</span>, where the residuals are defined by respectively subtracting the conditional expectation of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span> and <span class="math inline">\(D\)</span> given <span class="math inline">\(X\)</span> from <span class="math inline">\(Y\)</span> and <span class="math inline">\(D\)</span>. This result generalizes the FWL from linear models to partially linear models.</p></li>
<li><p>Our estimation procedure for <span class="math inline">\(\alpha\)</span> in the sample will mimic the partialling out procedure in the population. We also rely on cross-fitting (outlined below) to make sure our estimated residualized quantities are not overfit.</p></li>
</ul>
</section>
<section id="doubleorthogonal-ml-for-the-partially-linear-model" class="slide level2">
<h2>Double/Orthogonal ML for the Partially Linear Model</h2>
<ol type="1">
<li><p>Partition data indices into random folds of approximately equal size: <span class="math inline">\(\{1,...,n\} = \cup_{k=1}^K I_k\)</span>. For each fold <span class="math inline">\(k=1,...,K\)</span>, compute ML estimators <span class="math inline">\(\hat \ell_{[k]}\)</span> and <span class="math inline">\(\hat m_{[k]}\)</span> of the conditional expectation functions <span class="math inline">\(\ell\)</span> and <span class="math inline">\(m\)</span>, leaving out the <span class="math inline">\(k\)</span>-th block of data. Obtain the cross-fitted residuals for each <span class="math inline">\(i \in I_k\)</span>: <span class="math display">\[
\check Y_i = Y_i  - \hat \ell_{[k]}(X_i),   \quad \check D_i = D_i - \hat m_{[k]}(X_i).
\]</span></p></li>
<li><p>Apply ordinary least squares of <span class="math inline">\(\check Y_i\)</span> on <span class="math inline">\(\check D_i\)</span>, that is, obtain the <span class="math inline">\(\hat \alpha\)</span> as the root in <span class="math inline">\(a\)</span> of the normal equations: <span class="math display">\[
\mathbb{E}_n[(\check Y_i -  a \check D_i) \check D_i]=0.
\]</span></p></li>
<li><p>Construct standard errors and confidence intervals as in standard least squares theory.</p></li>
</ol>
</section>
<section class="slide level2">

<ul>
<li><p>In what follows it will be convenient to use the notation <span class="math display">\[
\| h \|_{L^2} := \sqrt{\mathbb{E}_X[h^2(X)]},
\]</span> where <span class="math inline">\(\mathbb{E}_X\)</span> computes the expectation over values of <span class="math inline">\(X\)</span>.</p></li>
<li><p>To do valid inference on the target parameter, is crucial that estimators <span class="math inline">\(\hat \ell_{[k]}(X)\)</span> and <span class="math inline">\(\hat m_{[k]}(X)\)</span> provide approximations to the best predictors <span class="math inline">\(\ell(X)\)</span> and <span class="math inline">\(m(X)\)</span> that are of sufficiently high quality: <span class="math display">\[
n^{1/4}( \| \hat \ell_{[k]} - \ell\|_{L^2} +  \| \hat m_{[k]} - m\|_{L^2}) \approx 0.
\]</span></p></li>
<li><p>This implies that both estimators <span class="math inline">\(\hat \ell\)</span> and <span class="math inline">\(\hat m\)</span> convergences at least at rate <span class="math inline">\(o(n^{-1/4})\)</span> to the true nuisance functions <span class="math inline">\(\ell\)</span> and <span class="math inline">\(m\)</span> measured in <span class="math inline">\(L^2\)</span>-norm.</p></li>
</ul>
</section>
<section id="adaptive-inference-on-a-target-parameter-in-plm" class="slide level2">
<h2>Adaptive Inference on a Target Parameter in PLM</h2>
<div id="thm-DML" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2 </strong></span>Consider the PLM model in <a href="#/dml-inference-in-the-partially-linear-regression-model-plm">Equation&nbsp;1</a>. Suppose that <span class="math inline">\(\mathbb{E}[\tilde D^2]\)</span> is bounded away from zero and that <span class="math display">\[
n^{1/4}( \| \hat \ell_{[k]} - \ell\|_{L^2} +  \| \hat m_{[k]} - m\|_{L^2}) \approx 0.\]</span> Suppose further regularity conditions listed in <span class="citation" data-cites="DML">Chernozhukov et al. (<a href="#/bibliography" role="doc-biblioref" onclick="">2018</a>)</span>.</p>
<p>Then the estimation error in <span class="math inline">\(\check D_i\)</span> and <span class="math inline">\(\check Y_i\)</span> has no first order effect on <span class="math inline">\(\hat\alpha\)</span>: <span class="math display">\[
\sqrt{n} (\hat\alpha- \alpha)  \approx  \mathbb{E}_n[\tilde D^2]^{-1} \sqrt{n} \mathbb{E}_n[\tilde D \epsilon].
\]</span></p>
</div>
</section>
<section class="slide level2">

<ul>
<li><p>Consequently, <span class="math inline">\(\hat \alpha\)</span> concentrates in a <span class="math inline">\(1/\sqrt{n}\)</span> neighborhood of <span class="math inline">\(\alpha\)</span> with deviations approximated by the Gaussian law: <span class="math display">\[
\sqrt{n} (\hat \alpha - \alpha)  \overset{{\mathrm{D}}}{\rightarrow}  N(0 ,  \mathsf{V}),
\]</span> where <span class="math display">\[
\mathsf{V} = (\mathbb{E} \tilde D^2)^{-1}  \mathbb{E} (\tilde D^2 \epsilon^2 )  (\mathbb{E} \tilde D^2)^{-1}.
\]</span></p></li>
<li><p><strong>Confidence Interval</strong>: The standard error of <span class="math inline">\(\hat \alpha\)</span> is <span class="math inline">\(\sqrt{\hat{\mathsf{V}}/n}\)</span>, where <span class="math inline">\(\hat{\mathsf{V}}\)</span> is an estimator of <span class="math inline">\(V\)</span>. The result implies that the confidence interval <span class="math display">\[
\left[\hat \alpha - 2 \sqrt{\hat{\mathsf{V}}/n}, \hat \alpha + 2\sqrt{\hat{\mathsf{V}}/n}\right]
\]</span> covers <span class="math inline">\(\alpha\)</span> in approximately <span class="math inline">\(95\%\)</span> of possible realizations of the sample. In other words, if our sample is not atypical, the interval covers the truth.</p></li>
</ul>
</section>
<section id="selecting-the-best-ml-learners-of-ell-and-m." class="slide level2">
<h2>Selecting the Best ML Learners of <span class="math inline">\(\ell\)</span> and <span class="math inline">\(m\)</span>.</h2>
<ul>
<li><p>There may be several methods that satisfy the quality requirements of <a href="#/adaptive-inference-on-a-target-parameter-in-plm">Theorem&nbsp;2</a>, and we may therefore ask what ML methods we should use in practice. Our goal would be to select the methods that minimize an upper bound on the bias of the DML estimator.</p></li>
<li><p>The bias of the DML estimator is controlled by the mean square approximation errors (MSAE): <span class="math display">\[\begin{equation}\label{eq: errors}
\frac{1}{K} \sum_{k=1}^K  \| \hat \ell_{[k]} - \ell\|^2_{L^2} \text { and } \frac{1}{K} \sum_{k=1}^K  \| \hat m_{[k]} - m \|^2_{L^2}.
\end{equation}\]</span> Therefore, we can select the best ML method for estimating <span class="math inline">\(m\)</span> and the best method for estimating <span class="math inline">\(\ell\)</span> to minimize the upper bound on the bias. We will be using mean square prediction errors as proxies for MSAEs.</p></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><p>Two different ML methods may be the best for predicting <span class="math inline">\(Y\)</span> and predicting <span class="math inline">\(D\)</span>. By doing MSPE minimization we in fact minimize MSAEs, since MSPEs approximate MSAEs plus terms that do not depend on <span class="math inline">\(j\)</span>.</p></li>
<li><p>Rather than selecting the single best predictors of <span class="math inline">\(Y\)</span> and <span class="math inline">\(D\)</span>, we can also use residuals to form linear ensembles of ML methods that minimize MSPEs.</p></li>
</ul>
</section>
<section id="discussion-of-dml-construction" class="slide level2">
<h2>Discussion of DML Construction</h2>
<ul>
<li><p>The partialling-out operation causes the moment equations defining <span class="math inline">\(\alpha\)</span> to be Neyman-orthogonal. That is, the moment conditions are insensitive to perturbations of the nuisance parameters <span class="math inline">\(\ell\)</span> and <span class="math inline">\(m\)</span>.<br>
</p></li>
<li><p>Generally we use the term nuisance parameters to name parameters that are not the target parameters. Here the target parameter is <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\ell\)</span> and <span class="math inline">\(m\)</span> are nuisance parameters.</p></li>
<li><p>We already discussed Neyman-orthogonality in the context of high-dimensional linear regression models in Lecture 4. We return to and generalize this discussion formally later in this lecture.</p></li>
<li><p>Naive application of machine learning methods directly to outcome equations may lead to highly biased estimators, because the resulting strategy is not Neyman-orthogonal.</p></li>
<li><p>The biases in estimation of <span class="math inline">\(g\)</span> in <a href="#/dml-inference-in-the-partially-linear-regression-model-plm">Equation&nbsp;1</a>, which are unavoidable in high-dimensional estimation, create a non-trivial bias in the estimate of the main effect. This bias is large enough to cause failure of conventional inference.</p></li>
</ul>
</section>
<section id="simulation-study-i" class="slide level2">
<h2>Simulation Study I</h2>

<img data-src="SIMULATEDML.png" class="r-stretch quarto-figure-center"><p class="caption">Figure&nbsp;1: Left: Behavior of a conventional (non-orthogonal) ML estimator. Right: Behavior of the orthogonal, DML estimator.</p></section>
<section class="slide level2">

<ul>
<li><p>The left panel in <a href="#/fig-orth">Figure&nbsp;1</a> illustrates the bias arising due to the use of a non-orthogonal, naive approach for learning <span class="math inline">\(\alpha\)</span>. Specifically, the figure shows the behavior of a conventional (non-orthogonal) ML estimator, <span class="math inline">\(\tilde \alpha\)</span>, in the partially linear model in a simple simulation experiment where we learn <span class="math inline">\(g\)</span> using a random forest. The <span class="math inline">\(g\)</span> in this experiment is a very smooth function of a small number of variables, so the experiment is seemingly favorable to the use of random forests a priori.</p></li>
<li><p>The histogram shows the simulated distribution of the centered estimator, <span class="math inline">\(\tilde \alpha - \alpha\)</span>. The estimator is badly biased, shifted much to the right relative to the true value <span class="math inline">\(\alpha\)</span>. Furthermore, the distribution of the estimator (approximated by the blue histogram) is substantively different from a normal approximation (shown by the red curve) derived under the assumption that the bias is negligible.</p></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><p>This biased performance of the naive estimator can also be explained analytically. The naive strategy relies on the moment equation: <span class="math display">\[
\mathbb{E}[(Y - \alpha D - g(X) ) D] = 0  
\]</span> to identify <span class="math inline">\(\alpha\)</span> and uses a biased estimate of <span class="math inline">\(g\)</span> in place of <span class="math inline">\(g\)</span>. This moment strategy is sensitive to deviations away from the true value. Indeed, let us compute the directional derivative in the direction <span class="math inline">\(\Delta\)</span> away from the true value: <span class="math display">\[
\partial_t \mathbb{E} [(Y - \alpha D - g(X) + t \Delta(X) ) D]  \Big |_{t=0} =  \mathbb{E}[\Delta (X) D]\neq 0.
\]</span></p></li>
<li><p>The derivative generally does not vanish, and the biases in estimation of <span class="math inline">\(g\)</span> will transmit to the estimation of <span class="math inline">\(\alpha\)</span>.</p></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><p>The right panel in <a href="#/fig-orth">Figure&nbsp;1</a> illustrates the behavior of the (Neyman) orthogonal DML estimator, <span class="math inline">\(\hat \alpha\)</span>, in the partially linear model in a simple experiment where we learn nuisance functions <span class="math inline">\(m\)</span> and <span class="math inline">\(\ell\)</span> using random forests. Note that the simulated data are exactly the same as those underlying the left panel. The simulated distribution of the centered estimator, <span class="math inline">\(\hat \alpha - \alpha\)</span>, (given by the blue histogram) illustrates that the estimator is approximately unbiased, concentrates around <span class="math inline">\(\alpha\)</span>, and is approximately normally distributed. The low bias arises because DML uses the Neyman-orthogonal moment equations.</p></li>
<li><p>The DML algorithm uses a form of sample splitting, called cross-fitting, to make sure our estimated residualized quantities are not overfit. Biases arising from overfitting could result from using highly complex fitting methods such as boosting, deep neural networks and random forests. If we don’t do sample splitting and the ML estimates overfit, we may end up with very large biases (see <a href="#/fig-crossf">Figure&nbsp;2</a>).</p></li>
<li><p>Again, this <a href="https://github.com/DoubleML/BasicsDML">shiny app</a> illustrate the problem of overfitting and the use of cross-fitting.</p></li>
</ul>
</section>
<section id="simulation-study-ii" class="slide level2">
<h2>Simulation Study II</h2>

<img data-src="contrivedSimIVResults.png" class="r-stretch quarto-figure-center"><p class="caption">Figure&nbsp;2: Left: DML distribution without sample-splitting. Right: DML distribution with cross-fitting.</p></section>
<section id="the-effect-of-gun-ownership-on-homicide-rates" class="slide level2">
<h2>The Effect of Gun Ownership on Homicide Rates</h2>
<ul>
<li><p>We consider the problem of estimating the effect of gun ownership on the homicide rate. For this purpose, we estimate the partially linear model: <span class="math display">\[
Y_{j,t} = \alpha D_{j,(t-1)} + g(Z_{j,t}) + \epsilon_{j,t}.
\]</span></p></li>
<li><p><span class="math inline">\(Y_{j,t}\)</span> is the log homicide rate in county <span class="math inline">\(j\)</span> at time <span class="math inline">\(t\)</span>. <span class="math inline">\(D_{j, t-1}\)</span> is the log fraction of suicides committed with a firearm in county <span class="math inline">\(j\)</span> at time <span class="math inline">\(t-1\)</span>, which we use as a proxy for gun ownership <span class="math inline">\(G_{j,t}\)</span>, which is not observed. <span class="math inline">\(Z_{j,t}\)</span> is a set of demographic and economic characteristics of county <span class="math inline">\(j\)</span> at time <span class="math inline">\(t\)</span>.</p></li>
<li><p>Control variables <span class="math inline">\(Z_{j,t}\)</span> are from the U.S. Census Bureau and contain demographic and economic characteristics of the counties such as the age distribution, the income distribution, crime rates, federal spending, home ownership rates, house prices, educational attainment, voting patterns, employment statistics, and migration rates.</p></li>
<li><p>The intent here is that parameter <span class="math inline">\(\alpha\)</span> is an approximation of the causal effect of gun ownership <span class="math inline">\(G_{j,t}\)</span> on homicide rates <span class="math inline">\(Y_{j,t}\)</span>, controlling for county-level demographic and economic characteristics.</p></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><p>To account for fixed heterogeneity across counties and time trends in all variables, we have removed county-specific and time-specific effects from all variables prior to estimation. The sample covers 195 large United States counties between the years 1980 through 1999, giving us 3900 observations.</p></li>
<li><p>An other way to flexibly account for fixed heterogeneity across counties, common time factors, and deterministic time trends, we might also include county-level averages, time period averages, initial conditions, and the time index as additional control variables. This strategy is related to strategies for addressing latent sources of heterogeneity via conditioning as in <span class="citation" data-cites="wooldridge:TWFE">Wooldridge (<a href="#/bibliography" role="doc-biblioref" onclick="">2021</a>)</span>.</p></li>
<li><p>Finally, for simplicity in this illustration, we assume that all sources of dependence are accounted for by observed variables such that we may take <span class="math inline">\(\epsilon_{j,t}\)</span> as independent across counties, <span class="math inline">\(j\)</span>, and over time, <span class="math inline">\(t\)</span>.</p></li>
</ul>
</section>
<section class="slide level2">

<!-- ![Causal DAG with variables $G_{j,t}$, $D_{j,t}$, $X_{j,t}$, and $Y_{jt}$.](dag8.2.png) -->

<img data-src="dag8.2.png" class="r-stretch quarto-figure-center"><p class="caption">A Possible DAG Structure for the Gun Ownership Example.</p></section>
<section class="slide level2">

<ul>
<li><p>Here we approximate the average causal effect <span class="math inline">\(G_{j,t} \to Y_{jt}\)</span> only if <span class="math inline">\(G_{j,t} \approx D_{j,t-1}\)</span>. Under the assumption that <span class="math inline">\(D_{j,t-1}\)</span> is equal to <span class="math inline">\(G_{j,t}\)</span> plus an additive, independent measurement error, the target parameter <span class="math inline">\(\alpha\)</span> will be attenuated relative to the true causal effect.</p></li>
<li><p>We might also include nodes for latent county specific and time period specific shocks in our DAG. Often such shocks are accounted for with so-called “fixed effects” which typically leverage strong functional form assumptions. If we include county-level averages, time period averages, initial conditions, and the time index as additional control variables, we instead leverage the different, though still strong assumption that flexibly conditioning on observables, including time- and county- specific variables, is a valid identification strategy.</p></li>
</ul>
</section>
<section class="slide level2">


<img data-src="dag8.3.png" class="r-stretch quarto-figure-center"><p class="caption">A Possible DAG Structure with latent county specific and time period specific shocks for the Gun Ownership Example.</p></section>
<section class="slide level2">

<ul>
<li><p>As a summary statistic we first look at a simple regression of <span class="math inline">\(Y_{j,t}\)</span> on <span class="math inline">\(D_{j,t-1}\)</span> without controls. The point estimate is <span class="math inline">\(0.282\)</span> with the confidence interval ranging from <span class="math inline">\(0.17\)</span> to <span class="math inline">\(0.39\)</span>. These results suggest that increases in gun ownership rates are associated with (predicted) gun homicide rates – if gun ownership increases by <span class="math inline">\(1\%\)</span> relative to the estimated trend then the predicted gun homicide rate goes up by <span class="math inline">\(0.28\%\)</span>, without controlling for time-varying county characteristics. Since our goal is to estimate the effect of gun ownership after controlling for a rich set of county characteristics, we next include the controls and estimate the model by an array of the modern regression methods that we’ve learned.</p></li>
<li><p>The Notebook of the empirial analysis can be found <a href="https://drive.google.com/file/d/1xfEUTnJzCNczZFMc5ElOqldbuxyATri_/view?usp=sharing">here</a>.</p></li>
</ul>
</section>
<section class="slide level2">

<table data-quarto-postprocess="true">
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th"></td>
<td data-quarto-table-cell-role="th">Estimate</td>
<td data-quarto-table-cell-role="th">Standard Error</td>
</tr>
<tr class="even">
<td>Baseline OLS</td>
<td>0.282</td>
<td>0.065</td>
</tr>
<tr class="odd">
<td>Least Squares with controls</td>
<td>0.191</td>
<td>0.052</td>
</tr>
<tr class="even">
<td>Lasso</td>
<td>0.223</td>
<td>0.057</td>
</tr>
<tr class="odd">
<td>Post-Lasso</td>
<td>0.227</td>
<td>0.056</td>
</tr>
<tr class="even">
<td>CV Lasso</td>
<td>0.200</td>
<td>0.058</td>
</tr>
<tr class="odd">
<td>CV Elnet</td>
<td>0.206</td>
<td>0.057</td>
</tr>
<tr class="even">
<td>CV Ridge</td>
<td>0.201</td>
<td>0.058</td>
</tr>
<tr class="odd">
<td>Random Forest</td>
<td>0.192</td>
<td>0.058</td>
</tr>
<tr class="even">
<td>Best</td>
<td>0.219</td>
<td>0.057</td>
</tr>
</tbody>
</table>
</section>
<section class="slide level2">

<ul>
<li><p>The table shows the estimated effects of the lagged gun ownership rate on the gun homicide rate as well as the standard error. We first focus on the Lasso method: The estimated effect is about <span class="math inline">\(.22\)</span>. This means that a <span class="math inline">\(1\%\)</span> increase in gun ownership rate (as measured by the proxy) leads to a predicted near quarter percent increase in gun homicide rates. The <span class="math inline">\(95\%\)</span> confidence interval for the effect ranges from <span class="math inline">\(0.12\)</span> to <span class="math inline">\(0.32\)</span>. These estimates are slightly higher than the ones obtained by the Least Squares Method. Random Forest also gives similar estimates to OLS, though with somewhat wider confidence bands.</p></li>
<li><p>The last row of the table provides the “best” estimates. To obtain “best” estimates we evaluate the performance of predictors <span class="math inline">\(\hat \ell(X)\)</span> and <span class="math inline">\(\hat m(X)\)</span> estimated by different methods on auxiliary samples using the main sample. Then we pick the methods giving the lowest MSE. In our case ridge regression and lasso give the best performances in predicting <span class="math inline">\(Y_{j,t}\)</span> and <span class="math inline">\(D_{j,t-1}\)</span>, respectively. We then use the best methods as predictors in the estimation procedure described above. The resulting estimate of the gun ownership effect and standard error are similar to that of Lasso.</p></li>
</ul>
</section>
<section id="dml-inference-in-the-interactive-regression-model-irm" class="slide level2">
<h2>DML Inference in the Interactive Regression Model (IRM)</h2>
<ul>
<li><p>We consider estimation of average treatment effects when treatment effects are fully heterogeneous and the treatment variable is binary. We consider vectors <span class="math inline">\(W=(Y,D,X)\)</span> and the pair of regression equations: <span id="eq-HetPL1"><span class="math display">\[
\begin{eqnarray}\label{eq: HetPL1}
&amp; Y  = g_0(D,X) + \epsilon,  &amp;  \quad \mathbb{E}[\epsilon \mid X, D]= 0, \\
&amp; D  = m_0(X) + \tilde D, \label{eq: HetPL2}  &amp; \quad  \mathbb{E}[\tilde D\mid X] = 0,
\end{eqnarray}
\qquad(2)\]</span></span> where the second regression equation is presented for convenience.</p></li>
<li><p>Here <span class="math inline">\(Y\)</span> is an outcome of interest, <span class="math inline">\(D \in \{0,1\}\)</span> is a binary policy or treatment variable, and <span class="math inline">\(X\)</span> are controls/confounding factors. Since <span class="math inline">\(D\)</span> is not additively separable in the first equation, this model is more general than the partially linear model for the case of binary <span class="math inline">\(D\)</span>.</p></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><p>A common target parameter of interest in this model is the average predictive effect (APE), <span class="math display">\[
\theta_0 = \mathbb{E}[ g_0(1,X) - g_0(0,X)].
\]</span> This quantity is the average predictive effect of switching <span class="math inline">\(D=0\)</span> to <span class="math inline">\(D=1\)</span>. Under conditional exogeneity discussed in Lecture 5 and Lecture 6, the APE coincides with the average treatment effect (ATE) of the intervention that moves <span class="math inline">\(D=0\)</span> to <span class="math inline">\(D=1\)</span>.</p></li>
<li><p>The confounding factors <span class="math inline">\(X\)</span> affect the policy variable via the propensity score <span class="math inline">\(m_0(X)\)</span> and the outcome variable via the function <span class="math inline">\(g_0(D,X)\)</span>. Both of these functions are unknown (except for the case of RCTs, where <span class="math inline">\(m_0(X)\)</span> is known) and potentially complicated, and we can employ ML methods to learn them.</p></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>Our construction of the efficient estimator for ATE will be based upon the relation <span id="eq-DMLIRM"><span class="math display">\[
\begin{equation}\label{eq:DML-IRM}
\theta_0 = \mathbb{E}[\varphi_0(W)],
\end{equation}
\qquad(3)\]</span></span> where <span class="math display">\[
\varphi_0(W) = g_0 (1,X) - g_0(0,X) + (Y- g_0(D,X)) H_0
\]</span> and <span class="math display">\[
H_0 = \frac{1(D=1)}{m_0(X)} -  \frac{1(D=0)}{1-m_0(X) }
\]</span> is the Horvitz-Thompson transformation.</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>We realize that this representation encompasses two equally valid representations of the target parameter: the regression adjusted representation, <span class="math display">\[
\theta_0 = \mathbb{E}[g_0 (1,X) - g_0(0,X)],\]</span> and the propensity score reweighting representation, <span class="math display">\[
\theta_0  =  \mathbb{E}[YH_0].
\]</span> Unfortunately <em>neither</em> of these representations is Neyman orthogonal, making them unsuitable for plugging-in machine learning estimators. In sharp contrast, the representation in <a href="#/eq-DMLIRM">Equation&nbsp;3</a> is Neyman orthogonal, which implies that we can readily deploy ML methods for estimation using the empirical analog of this expression coupled with cross-fitting.</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><p>The construction provided in <a href="#/eq-DMLIRM">Equation&nbsp;3</a> is equally applicable in cases where the propensity score <span class="math inline">\(\Pr(D=1|X)\)</span> is known, as in stratified randomized experiments, and in cases where the propensity score is unknown. When the propensity score is known, the role of regression adjustment in <a href="#/eq-DMLIRM">Equation&nbsp;3</a> is to reduce estimation noise.</p></li>
<li><p>We will employ the Neyman orthogonal parameterization and cross-fitting to construct a high-quality estimator and perform statistical inference on the target parameter.</p></li>
</ul>
</section>
<section id="dml-for-apesates-in-irm" class="slide level2">
<h2>DML for APEs/ATEs in IRM</h2>
<ol type="1">
<li><p>Partition sample indices into random folds of approximately equal size: <span class="math inline">\(\{1,...,n\} = \cup_{k=1}^K I_k\)</span>. For each <span class="math inline">\(k=1,...,K\)</span>, compute estimators <span class="math inline">\(\hat g_{[k]}\)</span> and <span class="math inline">\(\hat m_{[k]}\)</span> of the conditional expectation functions <span class="math inline">\(g_0\)</span> and <span class="math inline">\(m_0\)</span>, leaving out the <span class="math inline">\(k\)</span>-th block of data, such that <span class="math inline">\(\epsilon\leq \hat m_{[k]} \leq 1-\epsilon\)</span>, and for each <span class="math inline">\(i\in I_k\)</span> compute <span class="math display">\[
\hat \varphi (W_i) = \hat g_{[k]} (1,X_i) - \hat g_{[k]}(0,X_i) + (Y_i- \hat g_{[k]}(D_i,X_i)) \hat H_i
\]</span> with <span class="math inline">\(\hat H_i = \frac{1(D_i=1)}{\hat m_{[k]}(X_i)} - \frac{1(D_i=0)}{1-\hat m_{[k]}(X_i) }\)</span>.</p></li>
<li><p>Compute the estimator <span class="math inline">\(\hat\theta = \mathbb{E}_n[\hat \varphi (W_i)]\)</span>.</p></li>
<li><p>Construct standard errors via <span class="math display">\[
\sqrt{\hat{\mathsf{V}}/n}, \quad \hat{\mathsf{V}}= \mathbb{E}_n[(\hat \varphi(W_i) - \hat \theta)^2]\]</span> and use standard normal critical values for inference.</p></li>
</ol>
</section>
<section id="trimming" class="slide level2">
<h2>Trimming</h2>
<ul>
<li>An important practical issue is trimming <span class="math inline">\(|\hat H_i|\)</span> from taking explosively large values. Large values can occur when estimated propensity scores are near 0 or 1, which may indicate failure of the overlap condition (see Lecture 5) and restated in <a href="#/adaptive-inference-on-ate-with-dml">Theorem&nbsp;3</a> below. In the algorithm above, <span class="math inline">\(\hat H_i\)</span> can take on the largest absolute value of <span class="math inline">\(\bar H = 1/\epsilon\)</span>. Therefore, setting <span class="math inline">\(\epsilon =.01\)</span> corresponds to <span class="math inline">\(\bar H =100\)</span>. There does not seem to be a good theoretical or practical resolution on how to do trimming.</li>
</ul>
</section>
<section id="adaptive-inference-on-ate-with-dml" class="slide level2">
<h2>Adaptive Inference on ATE with DML</h2>
<div id="thm-DMLATE" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3 </strong></span>Suppose conditions specified in <span class="citation" data-cites="DML">Chernozhukov et al. (<a href="#/bibliography" role="doc-biblioref" onclick="">2018</a>)</span> hold. In particular, suppose that the overlap condition holds, namely for some <span class="math inline">\(\epsilon&gt;0\)</span> with probability 1 <span class="math inline">\(\epsilon&lt; m_0(X) &lt; 1-\epsilon\)</span>. If the estimators <span class="math inline">\(\hat g_{[k]}(D,X)\)</span> and <span class="math inline">\(\hat m_{[k]}(X)\)</span> <!-- estimators $\hat g_{[k]}(D,X)$ and $\hat m_{[k]}(X)$ are such that --> <!-- $\epsilon \leq \hat m_{[k]}(X)\leq  1-\epsilon$  and --> provide sufficiently high quality approximations to the best predictors <span class="math inline">\(g_0(D, X)\)</span> and <span class="math inline">\(m_0(X)\)</span> such that <span class="math display">\[
\| \hat g_{[k]}  - g_0  \|_{L^2} + \| \hat m_{[k]}  - m_0\|_{L^2}
+ \sqrt{n} \| \hat g_{[k]} - g_0\|_{L^2}  \| \hat m_{[k]} - m_0 \|_{L^2} \approx 0,
\]</span> then the estimation error in these nuisance parameter has no first order effect on <span class="math inline">\(\hat \theta\)</span>: <span class="math display">\[
\sqrt{n} (\hat \theta - \theta_0) \approx \sqrt{n} \mathbb{E}_n [\varphi_0(W) - \theta_0].
\]</span> Consequently, the estimator concentrates in <span class="math inline">\(1/\sqrt{n}\)</span> neighborhood of <span class="math inline">\(\theta_0\)</span>, with deviations controlled by the Gaussian law: <span class="math display">\[
\sqrt{n} (\hat \theta - \theta_0)  \overset{{\mathrm{D}}}{\rightarrow} N(0 ,  \mathsf{V})
\]</span> where <span class="math inline">\(\mathsf{V} = \mathbb{E}[(\varphi_0(W) - \theta_0)^2]\)</span>.</p>
</div>
</section>
<section class="slide level2">

<ul>
<li><p>The condition on the quality of estimators of <span class="math inline">\(g_0\)</span> and <span class="math inline">\(m_0\)</span> provides a possibility of “trading off” the quality of each estimator while retaining the adaptive inference property. The better we estimate the propensity score <span class="math inline">\(g_0\)</span>, the worse our estimate of the regression function <span class="math inline">\(m_0\)</span> can be; and vice versa.</p></li>
<li><p>We may also be interested in average effects for interesting subpopulations such as group ATEs (GATEs): <span class="math display">\[
\theta_0 = \mathbb{E}[ g_0(1,X) - g_0(0,X)|G=1],
\]</span><br>
where <span class="math inline">\(G\)</span> is a group indicator defined in terms of <span class="math inline">\(X\)</span>’s. For example, we might be interested in the impact of a vaccine on teenagers, in which case we could set <span class="math inline">\(G = 1(13 \leq Age \leq 19)\)</span>, or on older individuals, in which case we might set <span class="math inline">\(G = 1(65 \leq Age)\)</span>. DML estimation and inference for GATEs can be carried out similarly to estimation and inference for the ATE by exploiting the relation <span class="math display">\[
\theta_0 =\mathbb{E}[\varphi_0(X) |G =1]=  \mathbb{E} [\varphi_0(X) G]/\Pr(G=1).
\]</span></p></li>
</ul>
</section>
<section id="the-effect-of-401k-eligibility-on-net-financial-assets" class="slide level2">
<h2>The effect of 401(k) Eligibility on Net Financial Assets</h2>
<ul>
<li><p>Here we re-analyze the impact of 401(k) eligibility on financial assets (<span class="citation" data-cites="pvw:94">Poterba, Venti, and Wise (<a href="#/bibliography" role="doc-biblioref" onclick="">1994</a>)</span> and <span class="citation" data-cites="pvw:95">Poterba, Venti, and Wise (<a href="#/bibliography" role="doc-biblioref" onclick="">1995</a>)</span>). The data covers a short period a few years after the introduction of 401(k)’s when they were rapidly increasing in popularity.</p></li>
<li><p>The key problem in determining the effect of 401(k) eligibility is that working for a firm that offers access to a 401(k) plan is not randomly assigned. To overcome the lack of random assignment, we follow the strategy developed in <span class="citation" data-cites="pvw:94">Poterba, Venti, and Wise (<a href="#/bibliography" role="doc-biblioref" onclick="">1994</a>)</span> and <span class="citation" data-cites="pvw:95">Poterba, Venti, and Wise (<a href="#/bibliography" role="doc-biblioref" onclick="">1995</a>)</span>. In these papers, the authors use data from the 1991 Survey of Income and Program Participation and argue that eligibility for enrolling in a 401(k) plan in this data can be taken as exogenous after conditioning on a few observables of which the most important for their argument is income.</p></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><p>The basic idea of their argument is that, at least around the time 401(k)’s initially became available, people were unlikely to be basing their employment decisions on whether an employer offered a 401(k) but would instead focus on income and other aspects of the job. Following this argument, whether one is eligible for a 401(k) may then be taken as exogenous after appropriately conditioning on income and other control variables related to job choice.</p></li>
<li><p>A key component of the argument underlying the exogeneity of 401(k) eligibility is that eligibility may only be taken as exogenous after conditioning on income and other variables related to job choice that may correlate with whether a firm offers a 401(k). <span class="citation" data-cites="pvw:94">Poterba, Venti, and Wise (<a href="#/bibliography" role="doc-biblioref" onclick="">1994</a>)</span> and <span class="citation" data-cites="pvw:95">Poterba, Venti, and Wise (<a href="#/bibliography" role="doc-biblioref" onclick="">1995</a>)</span> and many subsequent papers adopt this argument but control for parsimonious, pre-specified functions of what they deem to be relevant characteristics. One might wonder whether such specifications are able to adequately control for income and other related confounders. At the same time, the power to learn about treatment effects decreases as one allows more flexible models. The principled use of flexible ML tools offers one resolution to this tension.</p></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>In what follows, we use net financial assets (the sum of IRA balances, 401(k) balances, checking accounts, U.S. saving bonds, other interest-earning accounts in banks and other financial institutions, other interest-earning assets (such as bonds held personally), stocks, and mutual funds less non-mortgage debt) as the outcome variable, <span class="math inline">\(Y\)</span>, in the analysis. The treatment variable, <span class="math inline">\(D\)</span>, is an indicator for being eligible to enroll in a 401(k) plan. The vector of raw covariates, <span class="math inline">\(X\)</span>, consists of age, income, family size, years of education, a married indicator, a two-earner status indicator, a defined benefit pension status indicator, an IRA participation indicator, and a home ownership indicator.</li>
</ul>
</section>
<section class="slide level2">


<img data-src="dag8.4.png" class="r-stretch quarto-figure-center"><p class="caption">Figure&nbsp;3: Three Causal DAGs for analysis of the 401(K) example in which adjusting for <span class="math inline">\(X\)</span> is a valid identification strategy. The bottom figure encompasses the other two as special cases.</p></section>
<section class="slide level2">

<ul>
<li><p>It is useful to think about a causal diagram that represents our thinking about identification in this example. In <a href="#/fig-401kDAGs">Figure&nbsp;3</a>, we provide three example DAGs for <span class="math inline">\(Y\)</span>, the outcome; <span class="math inline">\(D\)</span>, the 401(K) eligibility offer which depends on firm characteristics, <span class="math inline">\(F\)</span>, which are not observed; and <span class="math inline">\(X\)</span>, the worker characteristics. In one structure, <span class="math inline">\(F\)</span> determines the workers characteristics (via the hiring decision), so we have <span class="math inline">\(F \to X\)</span>. In another structure, workers determine the characteristics of the company they choose to work at, <span class="math inline">\(X \to F\)</span>. Finally, in the last structure <span class="math inline">\(F\)</span>, <span class="math inline">\(X\)</span>, and <span class="math inline">\(D\)</span> are jointly determined by a set of latent factors <span class="math inline">\(U\)</span>. In any of these cases, <span class="math inline">\(X\)</span> a valid adjustment set because it is the only parent of <span class="math inline">\(Y\)</span> (other than <span class="math inline">\(D\)</span>).</p></li>
<li><p>It is also useful to consider structures that would break down the identification strategy. We illustrate two such structures in <a href="#/fig-401kBadDAG1">Figure&nbsp;4</a> and <a href="#/fig-401kBadDAG2">Figure&nbsp;5</a>. In these figures, we introduce a node for the employer match amount <span class="math inline">\(M\)</span>, which could mediate the effect of 401(k) eligibility and have an important effect on financial wealth. Employers often offer a benefit where they will match a proportion of an employee’s contribution to their 401k, up to a limit. The limit is referred to as the employer match amount, and averages between 4 and 5% of employee’s salaries.</p></li>
</ul>
</section>
<section class="slide level2">


<img data-src="dag8.5.png" class="r-stretch quarto-figure-center"><p class="caption">Figure&nbsp;4: A DAG Structure where adjusting for <span class="math inline">\(X\)</span> is not sufficient. If there is no arrow from <span class="math inline">\(F\)</span> to <span class="math inline">\(M\)</span>, adjusting for <span class="math inline">\(X\)</span> is sufficient.</p></section>
<section class="slide level2">


<img data-src="dag8.6.png" class="r-stretch quarto-figure-center"><p class="caption">Figure&nbsp;5: Another DAG Structure where adjusting for <span class="math inline">\(X\)</span> is not sufficient. Here the latent confounder <span class="math inline">\(U\)</span> affects all variables, so even in the absence of an arrow connecting <span class="math inline">\(F\)</span> to <span class="math inline">\(M\)</span>, causal effects cannot be determined after adjusting for <span class="math inline">\(X\)</span>. The presence of such latent confounders is always a threat to causal interpretability of any observational study.</p></section>
<section class="slide level2">

<ul>
<li><p>In <a href="#/fig-401kBadDAG1">Figure&nbsp;4</a>, we suppose that <span class="math inline">\(M\)</span> is determined by unobserved firm characteristics, <span class="math inline">\(F\)</span>, and worker characteristics, <span class="math inline">\(X\)</span>. In this case, adjustment for <span class="math inline">\(X\)</span> is not sufficient as there is a path from latent firm characteristics, which are related to the treatment and to the outcome that is not closed by <span class="math inline">\(X\)</span>. However, if <span class="math inline">\(M\)</span> is determined solely by <span class="math inline">\(D\)</span> and <span class="math inline">\(X\)</span> so the red arrow is erased, adjustment for <span class="math inline">\(X\)</span> is sufficient. Therefore, interpreting our the target parameter of our estimation strategy as a causal effect is only valid if the match amount is independent of <span class="math inline">\(F\)</span> given <span class="math inline">\(D\)</span> and <span class="math inline">\(X\)</span>. Otherwise, the default interpretation is that we are estimating predictive effects of 401(k) eligibility.</p></li>
<li><p>In the second example, <a href="#/fig-401kBadDAG2">Figure&nbsp;5</a>, we maintain the assumption that <span class="math inline">\(M\)</span> is independent of <span class="math inline">\(F\)</span> given <span class="math inline">\(D\)</span> and <span class="math inline">\(X\)</span> by eliminating the arrow between nodes <span class="math inline">\(F\)</span> and <span class="math inline">\(M\)</span>. However, we now allow for the possibility that latent variables <span class="math inline">\(U\)</span> have a direct effect on <span class="math inline">\(Y\)</span>; that is, we have an unobserved confounder or omitted variable. In this example, such a counfounder may be unobserved risk preferences that relate to an individual’s preference over jobs, an individual’s characteristics, but also have direct effects on savings decisions not channeled purely through observed individual or job characteristics.</p></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><p>In general, the possibility of latent confounders always poses a challenge to obtaining estimates of causal effects in non-experimental data.</p></li>
<li><p>The presence or absence of latent confounders cannot be determined solely from the data in general, and thus their presence must be argued against based on scientific and institutional knowledge in different contexts. As in the previous example, we must interpret our estimates as predictive effects of 401(k) eligibility if we believe the connection from <span class="math inline">\(U\)</span> to <span class="math inline">\(Y\)</span> exists.</p></li>
<li><p>A detailed discussion of identification and potential DAGs can be found in this <a href="https://drive.google.com/file/d/10FNLvfr0rr58EKUTk8D8OisderIAl4ix/view?usp=sharing">notebook</a>.</p></li>
</ul>
</section>
<section id="empirical-results" class="slide level2">
<h2>Empirical Results</h2>

<table>
  <caption>Estimated Effect of 401(k) Eligibility on Net Financial Assets</caption>
  <thead>
    <tr>
      <th></th>
      <th>Lasso</th>
      <th>Random Forest</th>
      <th>Regression Tree</th>
      <th>Boosting</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td colspan="7"><em>A. Interactive Regression Model</em></td>
    </tr>
    <tr>
      <td>ATE</td>
      <td>8850</td>
      <td>8202</td>
      <td>8415</td>
      <td>8048</td>
    </tr>
    <tr>
      <td colspan="7"><em>B. Partially Linear Regression Model</em></td>
    </tr>
    <tr>
      <td>ATE</td>
      <td>9580</td>
      <td>9127</td>
      <td>8210</td>
      <td>8700</td>
    </tr>
    
  </tbody>
</table>
<p><strong>Note:</strong> Estimated ATE from a partially linear model (Panel B) and interactive regression model (Panel A) based on orthogonal estimating equations. Column labels denote the method used to estimate nuisance functions.</p>
</section>
<section class="slide level2">

<ul>
<li><p>The notebook of empirical analysis can be found <a href="https://colab.research.google.com/drive/1igC4zlBokApk39IUBiGKpbCYmDlqE3Yl?usp=sharing">here</a>.</p></li>
<li><p>It is first worth noting that when no controls are used, the estimated ATE of 401(k) eligibility on net financial assets is <strong>$19,559</strong>. Of course, this number is not a valid estimate of the causal effect of 401(k) eligibility on financial assets if there are neglected confounding variables as suggested by <span class="citation" data-cites="pvw:94">Poterba, Venti, and Wise (<a href="#/bibliography" role="doc-biblioref" onclick="">1994</a>)</span> and <span class="citation" data-cites="pvw:95">Poterba, Venti, and Wise (<a href="#/bibliography" role="doc-biblioref" onclick="">1995</a>)</span>.</p></li>
<li><p>When we turn to the estimates that flexibly account for confounding, we see that they are substantially attenuated relative to this baseline that does not account for confounding, suggesting much smaller causal effects of 401(k) eligibility on financial asset holdings.</p></li>
<li><p>The results obtained from the different machine learning methods are broadly consistent with each other. This similarity is consistent with the theory that suggests that results obtained through the use of orthogonal estimating equations and any method that provides sufficiently high-quality estimates of the necessary nuisance functions should be similar.</p></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><p>The results are also broadly consistent with those reported in the original work of and which used a simple, intuitively-motivated functional form, suggesting that this intuitive choice was sufficiently flexible to capture much of the confounding variation in this example.</p></li>
<li><p>So far we considered the effect of eligibility on net financial assets but one might also be interested in the effect of 401(k) participation on net financial assets which leads to the following model..</p></li>
</ul>
</section>
<section id="the-late-model" class="slide level2">
<h2>The LATE Model</h2>
<ul>
<li>An important nonlinear IV model is the local average treatment effect model (LATE), proposed by Imbens and Angrist (<span class="citation" data-cites="imbens:angrist:94">Imbens and Angrist (<a href="#/bibliography" role="doc-biblioref" onclick="">1994</a>)</span>).</li>
</ul>

<img data-src="dag8.7.png" class="r-stretch quarto-figure-center"><p class="caption">Figure&nbsp;6: Green arrow denotes a monotone functional relation.</p></section>
<section class="slide level2">

<ul>
<li>Consider the SEM, where</li>
</ul>
<p><span class="math display">\[
\begin{eqnarray*}
Y &amp;:=&amp;  f_Y (D, X, A, \epsilon_Y) \\
D &amp;:= &amp; f_D(Z, X, A, \epsilon_D) \in \{0,1\},  \\
Z  &amp;:= &amp; f_Z(X,\epsilon_Z) \in \{0,1\},  \\
X &amp;:=&amp;  \epsilon_X, \quad A = \epsilon_A,
\end{eqnarray*}
\]</span> where <span class="math inline">\(\epsilon\)</span>’s are all independent, and <span class="math display">\[
z \mapsto f_D(z , A, X, \epsilon_D) \text{ is weakly increasing (weakly monotone)}.
\]</span></p>
</section>
<section class="slide level2">

<ul>
<li><p>Suppose the instrument <span class="math inline">\(Z\)</span> is an offer to participate in a training program and that <span class="math inline">\(D\)</span> is the actual endogenous participation in the training program. Participation in the program may depend on unobservables <span class="math inline">\(A\)</span>, such as ability or perseverence, that also affect the eventual outcome <span class="math inline">\(Y\)</span>. We can also have background exogenous covariates <span class="math inline">\(X\)</span> in the model.</p></li>
<li><p>The model allows us to identify the local average treatment effect (LATE), defined as <span class="math display">\[
\theta = \mathbb{E}[Y(1) - Y(0) \mid D(1) &gt; D(0)],
\]</span> where <span class="math inline">\(\{D(1)&gt; D(0)\}\)</span> is the compliance event, where switching instrument value from <span class="math inline">\(Z=0\)</span> to <span class="math inline">\(Z=1\)</span> induces participation. Therefore LATE measures the average treatment effect conditional on compliance.</p></li>
</ul>
</section>
<section class="slide level2">

<p>In the LATE model, we have that <span class="math inline">\(\theta\)</span> is identified by the ratio of two statistical parameters, <span class="math display">\[
\theta  = \theta_1/\theta_2,\]</span> where <span class="math display">\[
\theta_1 :=  \mathbb{E}[\mathbb{E}[Y \mid X, Z=1] - \mathbb{E} [Y \mid X, Z=0] ],\]</span> and <span class="math display">\[
\theta_2 := \mathbb{E}[\mathbb{E}[D \mid X, Z=1] - \mathbb{E} [D \mid X, Z=0]],
\]</span> provided that the instrument <span class="math inline">\(Z\)</span> is relevant, <span class="math inline">\(\theta_2 &gt;0\)</span>, and <span class="math inline">\(Z\)</span> has full conditional support – namely <span class="math inline">\(0&lt; P(Z =1 |X)&lt;1\)</span>. Moreover, <span class="math inline">\(\theta_2\)</span> identifies the probability of compliance: <span class="math display">\[
\theta_2 = \Pr[D(1) &gt; D(0)].
\]</span></p>
</section>
<section id="inference-in-the-interactive-iv-regression-model-iivm" class="slide level2">
<h2>Inference in the Interactive IV Regression Model (IIVM)</h2>
<ul>
<li><p>In this section, we consider estimation of local average treatment effects (LATE) with a binary treatment variable, <span class="math inline">\(D\in\{0,1\}\)</span>, and a binary instrument, <span class="math inline">\(Z\in\{0,1\}\)</span>. As before, <span class="math inline">\(Y\)</span> denotes the outcome variable, and <span class="math inline">\(X\)</span> is the vector of covariates.</p></li>
<li><p>Consider the following statistical parameter:</p></li>
</ul>
<p><span class="math display">\[
\theta_0 = \frac{\mathbb{E}[\mathbb{E} (Y \mid Z=1, X) - \mathbb{E} (Y \mid Z=1, X) ]}{\mathbb{E}[\mathbb{E} (D \mid Z=1, X)- \mathbb{E} (D \mid  Z=0, X)]}.
\]</span></p>
<ul>
<li>This parameter is the ratio of the average predictive effects of <span class="math inline">\(Z\)</span> on <span class="math inline">\(Y\)</span> and of <span class="math inline">\(D\)</span> on <span class="math inline">\(Y\)</span>. Under the assumptions laid out on the previous slides this parameter is a causal parameter, i.e.&nbsp;the average treatment effect for compliers (LATE).</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>To set up estimation, define the regression functions: <span class="math display">\[\begin{align*}
&amp;  \mu_0(Z,X) = \mathbb{E}[ Y \mid Z, X] \\
&amp; m_0(Z,X) = \mathbb{E} [D \mid Z, X] \\
&amp; p_0(X) =  \mathbb{E} [Z \mid  X].
\end{align*}\]</span></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><p>Define the nuisance parameter <span class="math inline">\(\eta = (\mu, m, p)\)</span> to denote square-integrable functions <span class="math inline">\(\mu\)</span>, <span class="math inline">\(m\)</span>, and <span class="math inline">\(p\)</span>, with <span class="math inline">\(\mu\)</span> mapping the support of <span class="math inline">\((Z,X)\)</span> to <span class="math inline">\(\mathbb R\)</span> and <span class="math inline">\(m\)</span> and <span class="math inline">\(p\)</span> respectively mapping the support of <span class="math inline">\((Z,X)\)</span> and <span class="math inline">\(X\)</span> to <span class="math inline">\((\varepsilon, 1 - \varepsilon)\)</span> for some <span class="math inline">\(\varepsilon\in (0,1/2)\)</span>.</p></li>
<li><p>The true value of the nuisance parameter is <span class="math inline">\(\eta_0 = (\mu_0, m_0, p_0)\)</span>, the regression functions defined above.</p></li>
<li><p>The DML estimator of <span class="math inline">\(\theta_0\)</span> employs the orthogonal score <span class="math display">\[
\begin{align*}
\psi(W; \theta, \eta)
&amp; := \mu(1,X) - \mu(0,X) + H(p) (Y - \mu(Z,X))  \\
&amp; - \Big(m(1,X) - m(0,X) + H(p)  (D - m(Z,X) \Big) \theta,
\end{align*}
\]</span> for <span class="math inline">\(W = (Y,D,X,Z)\)</span> and <span class="math display">\[
H (p) :=   \frac{Z}{p(X)} - \frac{(1 - Z)}{1 - p(X)} .
\]</span></p></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><p>It is easy to verify (for homework) that this score satisfies the moment condition <span class="math display">\[\mathbb{E}[\psi(W; \theta_0,\eta_0)] = 0\]</span> and also the Neyman orthogonality condition <span class="math display">\[\partial_{\eta}\mathbb{E}[\psi(W; \theta_0,\eta_0)] = 0\]</span> at the true value <span class="math inline">\(\eta_0 = (\mu_0, m_0, p_0)\)</span> of the nuisance parameter.</p></li>
<li><p>Therefore we can apply the generic double ML algorithm to this problem, including the selection of the best ML methods to estimate the nuisance parameters.</p></li>
</ul>
</section>
<section class="slide level2">

<p>Suppose conditions specified in <span class="citation" data-cites="DML">Chernozhukov et al. (<a href="#/bibliography" role="doc-biblioref" onclick="">2018</a>)</span> hold. In particular, suppose that the overlap condition holds; namely, for some <span class="math inline">\(\epsilon&gt;0\)</span> with probability 1, <span class="math inline">\(\epsilon&lt; p_0(X) &lt; 1-\epsilon\)</span>. Further, suppose <span class="math inline">\(\epsilon &lt; \hat p_{[k]}(X)&lt; 1-\epsilon\)</span> and that estimators <span class="math inline">\(\hat p_{[k]}\)</span>, <span class="math inline">\(\hat m_{[k]}\)</span>, <span class="math inline">\(\hat \mu_{[k]}\)</span> provide high quality approximation to <span class="math inline">\(p_0\)</span>, <span class="math inline">\(m_0\)</span>, and <span class="math inline">\(mu_0\)</span> in the sense that <span class="math display">\[
n^{1/2} \|\hat  p_0 - p_0 \|_{L^2} \times \Big(\|\hat  \mu_0 - \mu_0 \|_{L^2} + \| \hat m_0 - m_0\|_{L^2}\Big) \approx 0.
\]</span> Then estimation of the nuisance parameters does not affect the behavior of the estimator to the first order; namely, <span class="math display">\[
\sqrt{n}(\hat \theta - \theta_0) \approx \sqrt{n} \mathbb{E}_n[\varphi_0(W)],
\]</span> where <span class="math display">\[
\varphi_0(W) = - J_0^{-1} \psi(W; \theta_0, \eta_0), \quad J_0 :=  \mathbb{E}[m_0(1,X) - m_0(0,X)].
\]</span></p>
</section>
<section class="slide level2">

<ul>
<li><p>Consequently, <span class="math inline">\(\hat \theta\)</span> concentrates in a <span class="math inline">\(1/\sqrt{n}\)</span>-neighborhood of <span class="math inline">\(\theta_0\)</span> and the sampling error <span class="math inline">\(\sqrt{n}(\hat \theta - \theta_0)\)</span> is approximately normal <span class="math display">\[
\sqrt{n}(\hat \theta - \theta_0) \overset{{\mathrm{D}}}{\rightarrow}  N(0, \mathsf{V}), \quad \mathsf{V}:=   \mathbb{E}[\varphi_0(W) \varphi_0(W)'].
\]</span></p></li>
<li><p>Variance estimation and confidence intervals are constructed as in the generic DML algorithm introduced later.</p></li>
</ul>
</section>
<section id="the-effect-of-401k-participation-on-net-financial-assets---continued" class="slide level2">
<h2>The effect of 401(k) Participation on Net Financial Assets - continued</h2>
<ul>
<li><p>Here we continue to re-analyze the effects of 401(k)’s on household financial asset.</p></li>
<li><p>In this section, we report the LATE in this example where we take the endogenous treatment variable to be <em>participating</em> in a 401(k) plan using 401(k) <em>eligibility</em> as instrument. Even after controlling for features related to job choice, it seems likely that the actual choice of whether to participate in an offered plan would be endogenous. Of course, we can use eligibility for a 401(k) plan as an instrument for participation in a 401(k) plan under the conditions that were used to justify the exogeneity of eligibility for a 401(k) plan.</p></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><p>We report DML results of estimating the LATE of 401(k) participation using 401(k) eligibility as an instrument in the next table. We employ the procedure outlined using the same ML estimators to estimate the quantities used to form the orthogonal estimating equation as we employed to estimate the ATE of 401(k) eligibility.</p></li>
<li><p>Looking at the results, we see that the estimated causal effect of 401(k) participation on net financial assets is uniformly positive and statistically significant across all of the considered methods.</p></li>
<li><p>As when looking at the ATE of 401(k) eligibility, it is reassuring that the results obtained from the different flexible methods are broadly consistent with each other.</p></li>
</ul>
</section>
<section id="dml-estimates-of-late-on-401k-participation-on-net-financial-assets" class="slide level2">
<h2>DML Estimates of LATE on 401(k) Participation on Net Financial Assets</h2>

<table>
  <thead>
    <tr>
      <th></th>
      <th>Lasso</th>
      <th>Random Forest</th>
      <th>Regression Tree</th>
      <th>Boosting</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Estimate</td>
      <td>12802</td>
      <td>11792</td>
      <td>12214</td>
      <td>11861 </td>
  </tr>
  </tbody><tbody>
    <tr>
</tr></tbody></table>
    
<ul>
<li>The results above based on flexible ML methods are broadly consistent with those obtained by applying the same specification for controls as used in <span class="citation" data-cites="pvw:94">Poterba, Venti, and Wise (<a href="#/bibliography" role="doc-biblioref" onclick="">1994</a>)</span> and <span class="citation" data-cites="pvw:95">Poterba, Venti, and Wise (<a href="#/bibliography" role="doc-biblioref" onclick="">1995</a>)</span> and using a linear IV model which returns an estimated effect of participation of <strong>$13,102</strong> (with slightly higher standard errors). The attenuation may suggest that the simple intuitive control specification used in the original baseline specification is not sufficiently flexible.</li>
</ul>
</section>
<section id="generic-debiased-or-double-machine-learning" class="slide level2">
<h2>Generic Debiased (or Double) Machine Learning</h2>
<ul>
<li>A general construction upon which DML estimation and inference can be built relies on a method-of-moments estimator for some low-dimensional target parameter <span class="math inline">\(\theta_0\)</span> based upon the empirical analog of the moment condition <span id="eq-moment"><span class="math display">\[
\begin{align}\label{eq: moment}
\mathbb{E}[\psi(W; \theta_0, \eta_0)] = 0,
\end{align}
\qquad(4)\]</span></span> where we call <span class="math inline">\(\psi\)</span> the score function, <span class="math inline">\(W\)</span> denotes a data vector, <span class="math inline">\(\theta_0\)</span> denotes the true value of a low-dimensional parameter of interest, and <span class="math inline">\(\eta\)</span> denotes nuisance parameters with true value <span class="math inline">\(\eta_0\)</span>.</li>
</ul>
</section>
<section id="the-first-key-input" class="slide level2">
<h2>The first key input</h2>
<ul>
<li><p>The first key input of the generic DML procedure is using a score function <span class="math inline">\(\psi(W; \theta, \eta)\)</span> such that <span class="math display">\[
\mathsf{M}(\theta, \eta) =  \mathbb{E}[\psi(W; \theta, \eta)]
\]</span> identifies <span class="math inline">\(\theta_0\)</span> when <span class="math inline">\(\eta = \eta_0\)</span>, that is, <span class="math display">\[\mathsf{M}(\theta, \eta_0)=0 \text{ if and only if } \theta = \theta_0\]</span> and the Neyman orthogonality condition is satisfied: <span id="eq-orthogonality"><span class="math display">\[
\begin{align}\label{eq: orthogonality}
\partial_\eta \mathsf{M}(\theta_0, \eta)  \Big |_{\eta = \eta_0} =  0.
\end{align}
\qquad(5)\]</span></span></p></li>
<li><p>Here, <a href="#/the-first-key-input">Equation&nbsp;5</a> ensures that the moment condition in <a href="#/generic-debiased-or-double-machine-learning">Equation&nbsp;4</a> used to identify and estimate <span class="math inline">\(\theta_0\)</span> is insensitive to small perturbations of the nuisance function <span class="math inline">\(\eta\)</span> around <span class="math inline">\(\eta_0\)</span>.</p></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><p>The orthogonality condition is named after Neyman (<span class="citation" data-cites="NeymanCalpha">Neyman (<a href="#/bibliography" role="doc-biblioref" onclick="">1959</a>)</span>), because he was the first to propose it in the context of parametric models with nuisance parameters that are estimated at slower than <span class="math inline">\(1/\sqrt{n}\)</span> rates.</p></li>
<li><p>Using a Neyman-orthogonal score eliminates the first order biases arising from the replacement of <span class="math inline">\(\eta_0\)</span> with a ML estimator <span class="math inline">\(\hat \eta_0\)</span>. Eliminating this bias is important because estimators <span class="math inline">\(\hat \eta_0\)</span> must be heavily regularized in high dimensional settings, so these estimators will be biased in general. The Neyman orthogonality property is responsible for the adaptivity of these estimators – namely, their approximate distribution will not depend on the fact that the estimate <span class="math inline">\(\hat \eta_0\)</span> contains error as long as the error is sufficiently mild.</p></li>
</ul>
</section>
<section id="the-second-key-input" class="slide level2">
<h2>The second key input</h2>
<ul>
<li><p>The second key input is the use of high-quality machine learning estimators of the nuisance parameters. A sufficient condition in the examples given includes the requirement <span class="math display">\[ n^{1/4} \| \hat \eta - \eta_0\|_{L^2} \approx 0.\]</span></p></li>
<li><p>Different structured assumptions on <span class="math inline">\(\eta_0\)</span> allow us to use different machine-learning tools for estimating <span class="math inline">\(\eta_0\)</span>. For instance approximate sparsity for <span class="math inline">\(\eta_0\)</span> in the linear case with respect to some dictionary calls for the use of lasso, post-lasso, or other sparsity-based techniques.</p></li>
</ul>
</section>
<section id="the-third-key-input" class="slide level2">
<h2>The third key input</h2>
<ul>
<li><p>The third key input is to use a form of sample splitting at the stage of producing the estimator of the target parameter <span class="math inline">\(\theta_0\)</span>, which allows us to avoid <em>biases</em> arising from overfitting.</p></li>
<li><p>Overfitting can easily occur when using highly complex fitting methods such as boosting, random forests, deep nets, ensembles, and other hybrid machine learning methods. We may heuristically think of overfitting as capturing noise that is particular to the observations used to fit a model in addition to signal. Using overfit estimates of nuisance parameters obtained using the same data as used to estimate the target parameter then heuristically leads to estimation error in these parameters being correlated to outcomes which introduces a type of bias.</p></li>
</ul>
</section>
<section id="the-dml-inference-method" class="slide level2">
<h2>The DML Inference Method</h2>
<p>We assume that we have a sample <span class="math inline">\((W_i)_{i=1}^n\)</span>, modeled as i.i.d. copies of data vector <span class="math inline">\(W\)</span>, whose law is determined by the probability measure <span class="math inline">\(P\)</span>. Recall that <span class="math inline">\(\mathbb{E}_n\)</span> denotes the empirical expectation: <span class="math display">\[\mathbb{E}_{n}[ g(W_i)] := \frac{1}{n} \sum_{i=1}^n g(W_i).\]</span> Let <span class="math inline">\(\mathbb{V}_{n}\)</span> denote the empirical variance: <span class="math display">\[
\mathbb{V}_{n}[ g(W_i)] :=   \mathbb{E}_{n}[g(W_i)g(W_i)'] -  \mathbb{E}_{n}[ g(W_i)]\mathbb{E}_{n}[ g(W_i)]'.
\]</span></p>
</section>
<section id="generic-dml-algortihm" class="slide level2">
<h2>Generic DML Algortihm</h2>
<p>Provide the data frame <span class="math inline">\((W_i)_{i=1}^n\)</span>, the Neyman-orthogonal score/moment function <span class="math inline">\(\psi(W, \theta, \eta)\)</span> that identifies the statistical parameter of interest, and the name and model for ML estimation method(s) for <span class="math inline">\(\eta\)</span>.</p>
<ol type="1">
<li><p><strong>Train ML Predictors on Folds:</strong> Take a K-fold random partition <span class="math inline">\((I_k)_{k=1}^K\)</span> of observation indices <span class="math inline">\(\{1,..., n\}\)</span> such that the size of each fold is about the same. For each <span class="math inline">\(k \in \{1,\ldots,K\}\)</span>, construct a high-quality machine learning estimator <span class="math inline">\(\hat \eta_{[k]}\)</span> that depends only on a subset of data <span class="math inline">\((X_i)_{i \not \in I_k}\)</span> that excludes the <span class="math inline">\(k\)</span>-th fold.</p></li>
<li><p><strong>Estimate Moments:</strong> Letting <span class="math inline">\(k(i) = \{ k : i \in I_k\}\)</span>, construct the moment equation estimate <span class="math display">\[
\hat{\mathsf{M}}(\theta, \hat \eta) =  \mathbb{E}_{n}[ \psi(W_i;  \theta, \hat \eta_{[k(i)]} ) ]
\]</span></p></li>
<li><p><strong>Compute the Estimator:</strong> Set the estimator <span class="math inline">\(\hat \theta\)</span> as the solution to the equation <span class="math display">\[\begin{equation}\label{eq:analog:smooth}
\hat{\mathsf{M}}(\hat \theta, \hat \eta)   = 0.
\end{equation}\]</span></p></li>
</ol>
</section>
<section class="slide level2">

<ol start="4" type="1">
<li><p><strong>Estimate Its Variance:</strong> Estimate the asymptotic variance of <span class="math inline">\(\hat \theta\)</span> by <span class="math display">\[
\hat{\mathsf{V}} =  \mathbb{V}_n[\hat \varphi(W_i)],
\]</span> where <span class="math display">\[
\hat \varphi(W_i) =  -\hat J_0^{-1}  \psi(W_i; \hat \theta, \hat \eta_{[k(i)]})
\]</span> and <span class="math display">\[
\hat J_0 := \partial_\theta \mathbb{E}_n [\psi (W_i; \hat \theta, \eta_{[k(i)]})].
\]</span></p></li>
<li><p><strong>Confidence Intervals:</strong> Form an approximate <span class="math inline">\((1-\alpha)\%\)</span> confidence interval for any functional <span class="math inline">\(\ell'\theta_0\)</span>, where <span class="math inline">\(\ell\)</span> is a vector of constants, as <span class="math display">\[\Big[\ell'\hat \theta \pm c  \sqrt{\ell'\hat{\mathsf{V}} \ell/ n}\Big],\]</span> where <span class="math inline">\(c\)</span> is the <span class="math inline">\((1-\alpha/2)\)</span> quantile of <span class="math inline">\(N(0,1)\)</span>.</p></li>
</ol>
</section>
<section id="general-algorithm-with-cross-fitting" class="slide level2">
<h2>General Algorithm with Cross-Fitting</h2>
<iframe data-external="1" src="https://www.youtube.com/embed/BMAr27rp4uA?autoplay=1" width="1000" height="600" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</section>
<section id="further-material-and-notebooks" class="slide level2">
<h2>Further Material and Notebooks</h2>
<ul>
<li><p>An online tutorial on DoubleML for double/debiased machine learning in R and Python can be found <a href="https://www.youtube.com/watch?v=ErecsyKEq74">here</a>.</p></li>
<li><p>Notebook with exercises <a href="https://drive.google.com/file/d/1zQi_EQH75Gsfzl3SCfzKXF7lEWorcEVT/view?usp=sharing">DML Gun Ownership Exercises</a>.</p></li>
<li><p>Notebook on <a href="https://drive.google.com/file/d/1xfEUTnJzCNczZFMc5ElOqldbuxyATri_/view?usp=sharing">Gun Ownership</a>.</p></li>
<li><p>Notebook on <a href="https://colab.research.google.com/drive/1igC4zlBokApk39IUBiGKpbCYmDlqE3Yl?usp=sharing">4O1(k) eligibility/participation</a>.</p></li>
<li><p>Notebook on <a href="https://drive.google.com/file/d/10FNLvfr0rr58EKUTk8D8OisderIAl4ix/view?usp=sharing">identification</a> in 4O1(k) example.</p></li>
<li><p>The cross-fitting animation can be found <a href="https://www.youtube.com/watch?v=BMAr27rp4uA?autoplay=1">here</a>.</p></li>
</ul>
</section>
<section id="bibliography" class="slide level2 smaller scrollable">
<h2>Bibliography</h2>
<div class="footer footer-default">
<p>CML</p>
</div>
<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-DML" class="csl-entry" role="listitem">
Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. 2018. <span>“Double/Debiased Machine Learning for Treatment and Structural Parameters.”</span> <em>The Econometrics Journal</em> 21 (1): C1–68.
</div>
<div id="ref-imbens:angrist:94" class="csl-entry" role="listitem">
Imbens, Guido W., and Joshua D. Angrist. 1994. <span>“Identification and Estimation of Local Average Treatment Effects.”</span> <em>Econometrica</em> 62 (2): 467–75.
</div>
<div id="ref-NeymanCalpha" class="csl-entry" role="listitem">
Neyman, Jerzy. 1959. <span>“Optimal Asymptotic Tests of Composite Hypotheses.”</span> <em>Probability and Statsitics</em>, 213–34.
</div>
<div id="ref-pvw:94" class="csl-entry" role="listitem">
Poterba, James M., Steven F. Venti, and David A. Wise. 1994. <span>“401(k) Plans and Tax-Deferred Savings.”</span> In <em>Studies in the Economics of Aging</em>, edited by D. A. Wise, 105–42. Chicago, IL: University of Chicago Press.
</div>
<div id="ref-pvw:95" class="csl-entry" role="listitem">
———. 1995. <span>“Do 401(k) Contributions Crowd Out Other Personal Saving?”</span> <em>Journal of Public Economics</em> 58 (1): 1–32.
</div>
<div id="ref-wooldridge:TWFE" class="csl-entry" role="listitem">
Wooldridge, Jeffrey M. 2021. <span>“Two-Way Fixed Effects, the Two-Way Mundlak Regression, and Difference-in-Differences Estimators.”</span> <em>Available at SSRN: Https://Ssrn.com/Abstract=3906345 or Http://Dx.doi.org/10.2139/Ssrn.3906345</em>.
</div>
</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="L8_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="L8_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="L8_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="L8_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="L8_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="L8_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="L8_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="L8_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="L8_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="L8_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="L8_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true,"boardmarkerWidth":5,"theme":"chalkboard"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>