<!DOCTYPE html>
<html lang="en"><head>
<script src="L3_files/libs/clipboard/clipboard.min.js"></script>
<script src="L3_files/libs/quarto-html/tabby.min.js"></script>
<script src="L3_files/libs/quarto-html/popper.min.js"></script>
<script src="L3_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="L3_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="L3_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="L3_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="L3_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.5.57">

  <title>Lecture 3</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="L3_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="L3_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="L3_files/libs/revealjs/dist/theme/quarto.css">
  <link href="L3_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="L3_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="L3_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="L3_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="L3_files/libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="L3_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Lecture 3</h1>
  <p class="subtitle">HHU summer term 2025 <br> May 7, 2025 <br> Prof.&nbsp;Dr.&nbsp;Jannis Kück</p>

<div class="quarto-title-authors">
</div>

</section>
<section>
<section id="machine-learning-methods" class="title-slide slide level1 center">
<h1>Machine Learning Methods</h1>

</section>
<section id="linear-regression-with-high-dimensional-covariates" class="slide level2">
<h2>Linear Regression with High-Dimensional Covariates</h2>
<ul>
<li><p>We consider a regression model <span class="math display">\[
Y = \beta' X + \epsilon, \quad \epsilon \perp X,
\]</span> where <span class="math inline">\(\beta'X\)</span> is the population best linear predictor of <span class="math inline">\(Y\)</span> using <span class="math inline">\(X\)</span>, or simply the population linear regression function.</p></li>
<li><p>The vector <span class="math inline">\(X = (X_j)_{j=1}^p\)</span> is <span class="math inline">\(p\)</span>-dimensional. That is, there are <span class="math inline">\(p\)</span> regressors, and <span class="math display">\[
p \textbf{ is large, possibly much larger than }  n.
\]</span></p></li>
<li><p>This case where <span class="math inline">\(p\)</span> is very large is what we call a “high-dimensional” setting.</p></li>
<li><p>High-dimensional settings arise when data have large dimensional features (i.e.&nbsp;many covariates are available for use as regressors), or we construct many technical regressors. A <em>technical regressor</em> is any variable obtained as a transformation of a basic regressor from raw regressors.</p></li>
</ul>
</section>
<section id="constructed-regressors" class="slide level2">
<h2>Constructed Regressors</h2>
<ul>
<li><p>Examples of datasets where many covariates are available and potential corresponding exemplary applications include country characteristics in cross-country wealth analysis, housing characteristics in house pricing/appraisal analysis, individual health information in electronic health records and claims data, and product characteristics at the point of purchase in demand analysis.</p></li>
<li><p>Another source of high-dimensionality is the use of constructed regressors. If <span class="math inline">\(W\)</span> are “raw” regressors, <em>technical (constructed) regressors</em> are of the form <span class="math display">\[
X = P(W) = (P_1(W),..., P_p (W))',
\]</span> where the set of transformations <span class="math inline">\(P(W)\)</span> is sometimes called the “dictionary” of transformations.</p></li>
<li><p>Example transformations include polynomials, interactions between variables, and applying functions such as the logarithm or exponential.</p></li>
</ul>
</section>
<section id="motivation" class="slide level2">
<h2>Motivation</h2>
<ul>
<li><p>In the wage analysis in Lecture 1, for example, we used quadratic and cubic transformations of experience, as well as interactions (products) of these regressors with education and geographic indicators.</p></li>
<li><p>The main motivation for the use of constructed regressors is to build <strong>more flexible and potentially better</strong> prediction rules.</p></li>
<li><p>The potential for improved prediction arises because we are using prediction rules <span class="math inline">\(\beta'X=\beta'P(W)\)</span> that are <strong>nonlinear</strong> in the original raw regressors <span class="math inline">\(W\)</span> and may thus capture more complex patterns that exist in the data.</p></li>
<li><p>Conveniently, the prediction rule <span class="math inline">\(\beta'X\)</span> is still linear with respect to the parameters, <span class="math inline">\(\beta\)</span>, and with respect to the constructed regressors <span class="math inline">\(X = P(W)\)</span>, so inherits much from the previous discussion of linear regression provided in Lecture 1.</p></li>
</ul>
</section>
<section id="best-predictor" class="slide level2">
<h2>Best Predictor</h2>
<ul>
<li><p>In the population, the <strong>best predictor</strong> of <span class="math inline">\(Y\)</span> given <span class="math inline">\(W\)</span> is <span class="math display">\[g(W) = \mathbb{E}[Y \mid W],\]</span> the <strong>conditional expectation</strong> of <span class="math inline">\(Y\)</span> given <span class="math inline">\(W\)</span>. The function <span class="math inline">\(g(W)\)</span> is called the <strong>regression function</strong> of <span class="math inline">\(Y\)</span> on <span class="math inline">\(W\)</span>.</p></li>
<li><p>Specifically, the conditional expectation function <span class="math inline">\(g(W)\)</span> solves the best prediction problem <span class="math display">\[
  \min_{m(W)} \mathbb{E}[(Y - m(W))^2].
  \]</span></p></li>
<li><p>Here we minimize the mean squared prediction error (MSE) among all prediction rules <span class="math inline">\(m(W)\)</span> (linear or nonlinear in <span class="math inline">\(W\)</span>).</p></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><p>As the conditional expectation solves the same problem as the best linear prediction rule among a larger class of candidate rules, the conditional expectation generally provides better predictions than the best linear prediction rule (unless the conditional expectation function turns out to be linear, in which case the conditional expectation and best linear prediction rule coincide).</p></li>
<li><p>By using <span class="math inline">\(\beta'P(W)\)</span> we are implicitly approximating the <strong>best predictor</strong> <span class="math inline">\(\mathbb{E}[Y|W]\)</span>.</p></li>
<li><p>It can be shown that the <strong>best linear predictor</strong> (BLP) <span class="math inline">\(\beta'P(W)\)</span> is the <strong>Best Linear Approximation (BLA)</strong> to the best predictor – the regression function <span class="math inline">\(g(W)\)</span>.</p></li>
<li><p>By using a richer and richer dictionary <span class="math inline">\(P(W)\)</span> of transformations, the BLA <span class="math inline">\(\beta'P(W)\)</span> approximates <span class="math inline">\(g(W)\)</span> better and better.</p></li>
</ul>
</section>
<section class="slide level2">

<div id="exm-approx" class="theorem example">
<p><span class="theorem-title"><strong>Example 1 (Approximating A Smooth Function with a Polynomial Dictionary)</strong></span> &nbsp;</p>
<ul>
<li><p>Suppose <span class="math inline">\(W \sim U(0,1)\)</span> and <span class="math inline">\(g(W) = \exp(4\cdot W)\)</span>.</p></li>
<li><p>We use <span class="math display">\[P(W) = \underbrace{(1, W,W^2, \ldots ,W^{p-1})'}_{p \text{ terms}}\]</span> to form the BLA/BLP, <span class="math display">\[\beta'P(W).\]</span></p></li>
<li><p>How can we approximate g(W) accurately?</p></li>
</ul>
</div>
</section>
<section class="slide level2">

<ul>
<li>With <span class="math inline">\(p=2\)</span> we get a linear in <span class="math inline">\(W\)</span> approximation to <span class="math inline">\(g(W)\)</span>. As the figure shows, the quality of this approximation is poor:</li>
</ul>

<img data-src="BLA-chunk-3-2.png" style="width:80.0%" class="r-stretch quarto-figure-center"><p class="caption">
Figure&nbsp;1: Linear Approximation
</p></section>
<section class="slide level2">

<ul>
<li>With <span class="math inline">\(p=3\)</span> we get a quadratic-in-<span class="math inline">\(W\)</span> approximation to <span class="math inline">\(g(W)\)</span>. Here, the approximation quality is markedly improved relative to <span class="math inline">\(p=2\)</span> though approximation errors are still clearly visible:</li>
</ul>

<img data-src="BLA-chunk-3-3.png" class="r-stretch quarto-figure-center"><p class="caption">
Figure&nbsp;2: quadratic approximation
</p></section>
<section class="slide level2">

<ul>
<li>With <span class="math inline">\(p=4\)</span> we get a cubic-in-<span class="math inline">\(W\)</span> approximation to <span class="math inline">\(g(W)\)</span>, and the quality of approximation appears to be excellent:</li>
</ul>

<img data-src="BLA-chunk-3-4.png" class="r-stretch quarto-figure-center"><p class="caption">
Figure&nbsp;3: cubic approximation
</p><ul>
<li>This simple example highlights the motivation for using nonlinear transformations of raw regressors in linear regression analysis.</li>
</ul>
</section>
<section id="regression" class="slide level2">
<h2>Regression</h2>
<ul>
<li><p>Recall that we are considering a regression model <span class="math display">\[
Y = \beta' X + \epsilon = \sum_{j=1}^p \beta_j X_j + \epsilon,  \quad \epsilon \perp X
\]</span> where <span class="math inline">\(p\)</span> is possibly much larger than <span class="math inline">\(n\)</span>.</p></li>
<li><p>We further assume that regressors are normalized, <span class="math inline">\(\mathbb{E}[X^2_j]=1\)</span>, to discuss theoretical properties.</p></li>
<li><p>Classical linear regression or least squares fails in these high-dimensional settings because it overfits the data.</p></li>
<li><p>This is especially apparent when <span class="math inline">\(p \geq n\)</span>. We therefore make some assumptions and modify the regression method in order to deal with cases where <span class="math inline">\(p\)</span> is large.</p></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><p>An intuitive starting point is the assumption of <strong>approximate sparsity</strong>.</p></li>
<li><p>Under approximate sparsity, there is a small group of regressors with relatively large coefficients whose use alone suffices to approximate the BLP <span class="math inline">\(\beta'X\)</span> well.</p></li>
<li><p>The rest of the regressors are assumed to have relatively small coefficients and contribute little to the approximation of the BLP.</p></li>
</ul>
<!-- - An example of approximate sparsity is captured by regression coefficients of the form -->
<!-- $$\beta_j \propto 1/j^2, \quad j=1,...,p.$$   -->
<ul>
<li>Here, the first few coefficients capture almost all of the explanatory power of the full vector of coefficients as shown in the following figure:</li>
</ul>

<img data-src="unnamed-chunk-2-1.png" class="r-stretch quarto-figure-center"><p class="caption">
Figure&nbsp;4: Approximate Sparsity
</p></section>
<section id="approximate-sparsity" class="slide level2">
<h2>Approximate Sparsity</h2>
<ul>
<li>The sorted absolute values of the coefficients decay fast enough. Specifically, the j<span class="math inline">\(^{th}\)</span> largest coefficient (in absolute value) denoted by <span class="math inline">\(|\beta|_{(j)}\)</span> obeys <span class="math display">\[
\begin{equation}
|\beta|_{(j)} \leq  A j^{-a}, \quad  a&gt;1/2,\end{equation}
\]</span> for each <span class="math inline">\(j\)</span>, where the constants <span class="math inline">\(a\)</span> and <span class="math inline">\(A\)</span> do not depend on the sample size <span class="math inline">\(n\)</span>.</li>
</ul>
</section>
<section id="lasso" class="slide level2">
<h2>Lasso</h2>
<ul>
<li><p>Consider a random sample <span class="math inline">\((Y_i, X_i)_{i=1}^n\)</span>. We seek to construct a good linear predictor <span class="math inline">\(\hat \beta'X\)</span>, which works well when <span class="math inline">\(p/n\)</span> is not small.</p></li>
<li><p>Lasso constructs <span class="math inline">\(\hat \beta\)</span> as the solution of the following penalized least squares problem:</p></li>
</ul>
<p><span id="eq-lasso"><span class="math display">\[
\min_{b \in \mathbb{R}^p} \quad \sum_i (Y_i - b'X_i )^2 +  \lambda  \cdot \sum_{j=1}^p | b_j| \hat \psi_j ,
\qquad(1)\]</span></span> which is called the Lasso regression problem.</p>
<ul>
<li>The first term is <span class="math inline">\(n\)</span> times the sample mean squared error, and the second term is called a <em>penalty term</em>.</li>
</ul>
</section>
<section id="penalty" class="slide level2">
<h2>Penalty</h2>
<ul>
<li><p>The penalty term introduces a cost to the size of the prospective model where size is captured by the sum of the products of the absolute values of the coefficients <span class="math inline">\(b_j\)</span> with the <em>penalty loadings</em> <span class="math inline">\(\hat \psi_j\)</span> all multiplied by the <em>penalty level</em> <span class="math inline">\(\lambda\)</span>. The penalty loadings are typically set as <span class="math display">\[
\hat \psi_j = \sqrt{\mathbb{E}_n X^2_{ij}}.
\]</span></p></li>
<li><p>The use of this penalty ensures invariance of Lasso predictions to rescaling <span class="math inline">\(X_j'\)</span>. It is also desirable to demean <span class="math inline">\(X_j'\)</span>s other than the intercept, as this will ensure invariance of predictions to both location and scale transformations of <span class="math inline">\(X_j's\)</span>.</p></li>
<li><p>As long as <span class="math inline">\(\lambda &gt; 0\)</span>, the introduction of the penalty term in <a href="#/lasso" class="quarto-xref">Equation&nbsp;1</a> leads to a prediction rule which is less complex than the rule that would be obtained via solving the unpenalized least squares problem.</p></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><p>This preference for less complex models then helps guard against overfitting which can intuitively be understood as adding complexity to a prediction rule to capture some small variation in the sample that does not generalize out of sample.</p></li>
<li><p>A crucial point is thus the choice of the penalization parameter <span class="math inline">\(\lambda\)</span>. A theoretically valid choice is <span class="math display">\[ \lambda = 2 \cdot c \hat \sigma \sqrt{n} \Phi^{-1}( 1-\alpha/2p)\]</span> with <span class="math inline">\(\hat \sigma \approx \sigma = \sqrt{\mathbb{E} \epsilon^2}\)</span> obtained via an iteration method.</p></li>
<li><p><span class="math inline">\(\Phi^{-1}\)</span> denotes the quantile function (inverse) of the distribution function the standard normal variable <span class="math inline">\(N(0,1)\)</span>, <span class="math inline">\(\Phi(z) = P (N(0,1) \leq z)\)</span> and <span class="math inline">\(c&gt;1\)</span> and <span class="math inline">\(1-\alpha\)</span> is a confidence level.</p></li>
<li><p>This penalty level ensures that the Lasso predictor <span class="math inline">\(\hat \beta'X\)</span> does not overfit the data and delivers good predictive performance under approximate sparsity (<span class="citation" data-cites="BickelRitovTsybakov2009">Bickel, Ritov, and Tsybakov (<a href="#/bibliography" role="doc-biblioref" onclick="">2009</a>)</span>,<span class="citation" data-cites="BC-PostLASSO">Belloni and Chernozhukov (<a href="#/bibliography" role="doc-biblioref" onclick="">2013</a>)</span>).</p></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><p>Another good way to pick the penalty level is by cross-validation. Cross-validation is a form of a repeated data-splitting method to choose penalty parameters for Lasso and to choose among predictive models more generally. We outline the basic idea of cross-validation later.</p></li>
<li><p>Approximate sparsity is produced because the penalty function has a kink at zero, so the marginal cost of including regressor <span class="math inline">\(X_j\)</span> is always positive.</p></li>
<li><p>Therefore, Lasso includes a regressor <span class="math inline">\(X_j\)</span> only if its marginal predictive ability is higher than this threshold. We explain this point and how this feature of Lasso means that Lasso does variable selection in more detail below.</p></li>
</ul>
</section>
<section id="example" class="slide level2">
<h2>Example</h2>
<p>Consider <span class="math display">\[ Y = \beta'X + \varepsilon, \ \ X \sim N(0,I_p), \ \ \varepsilon \sim N(0,1),\]</span> with approximately sparse regression coefficients: <span class="math display">\[ \beta_j = 1/j^2,  \quad j=1,...,p\]</span> and <span class="math display">\[
n = 300, \quad p=1000.
\]</span></p>
</section>
<section class="slide level2">


<img data-src="unnamed-chunk-3-1.png" class="r-stretch quarto-figure-center"><p class="caption">
Figure&nbsp;5: The true coefficients (black) vs.&nbsp;coefficients estimated by Lasso (blue)
</p></section>
<section class="slide level2">

<ul>
<li><p><a href="#/fig-approxlasso" class="quarto-xref">Figure&nbsp;5</a> shows that <span class="math inline">\(\hat \beta\)</span> is sparse and is close to <span class="math inline">\(\beta\)</span>. We see that Lasso sets most of regression coefficients to zero. It figures out approximately the right set of regressors, including only those with the two largest coefficients.</p></li>
<li><p>Lasso also shrunks relevant regressors towards zero and “underestimates” the absolute value of the coefficients.</p></li>
</ul>
</section>
<section id="post-lasso" class="slide level2">
<h2>Post-Lasso</h2>
<ul>
<li><p>We can use the Lasso-selected set of regressors, those regressors whose Lasso coefficient estimates are non-zero, to refit the model by least squares.</p></li>
<li><p>This method is called “least squares post Lasso” or simply <em>Post-Lasso</em> (<span class="citation" data-cites="BC-PostLASSO">Belloni and Chernozhukov (<a href="#/bibliography" role="doc-biblioref" onclick="">2013</a>)</span>).</p></li>
<li><p>We define the Post-Lasso <span id="eq-postlasso"><span class="math display">\[
\widetilde \beta \in \arg\min_{\beta \in \mathbb{R}^p} \ \sum_i (Y_i-X_i'\beta)^2 \ \ :  \ \ \beta_j = 0  \text{ if } \hat \beta_j = 0,  \text{ for each } j,
\qquad(2)\]</span></span> where <span class="math inline">\(\hat \beta\)</span> is the Lasso coefficient estimator.</p></li>
<li><p>The formal properties of the Post-Lasso estimator <span class="math inline">\(\widetilde{\beta}\)</span> are similar to those of Lasso <span class="math inline">\(\hat{\beta}\)</span>, as recorded later.</p></li>
</ul>
</section>
<section class="slide level2">


<img data-src="unnamed-chunk-4-1.png" class="r-stretch quarto-figure-center"><p class="caption">
Figure&nbsp;6: The true coefficients (black) vs.&nbsp;coefficients estimated by Post-Lasso (blue)
</p></section>
<section id="predictive-performance-of-lasso-and-post-lasso" class="slide level2">
<h2>Predictive Performance of Lasso and Post-Lasso</h2>
<ul>
<li><p>Does <span class="math inline">\(\hat{\beta}'X\)</span> provide a good approximation to the best linear prediction rule (out-of-sample) <span class="math inline">\(\beta'X\)</span>?</p></li>
<li><p>Under approximate sparsity, only a few, say <span class="math inline">\(s\)</span>, parameters will be “important”. We can call <span class="math inline">\(s\)</span> the <strong>effective dimension</strong>.</p></li>
<li><p>Under exact <em>sparsity</em>, there are only <span class="math inline">\(s\)</span> non-zero coeffiencients.</p></li>
<li><p>Intuitively, to estimate each of the “important” <span class="math inline">\(s\)</span> parameters well, we need many observations for each such parameter. This means that <span class="math inline">\(n/s\)</span> must be large.</p></li>
<li><p>Using previous reasoning from least squares theory with <span class="math inline">\(s &lt; n\)</span> regressors, we might also conjecture that the key determinant of the rate at which Lasso approximates the best linear predictor is <span class="math inline">\(\sqrt{s/n}\)</span>.</p></li>
</ul>
</section>
<section class="slide level2">

<div id="thm-lasso" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 (Prediction Rate)</strong></span> Under the approximate sparsity assumption and other regularity conditions stated e.g.&nbsp;in <span class="citation" data-cites="BC-PostLASSO">Belloni and Chernozhukov (<a href="#/bibliography" role="doc-biblioref" onclick="">2013</a>)</span>, with probability approaching <span class="math inline">\(1-\alpha\)</span> as <span class="math inline">\(n \to \infty\)</span>, the following bound holds: <span class="math display">\[
\sqrt{\mathbb{E}[( \beta'X - \hat\beta'X)^2}] \leq  \mathrm{const}   \cdot \sqrt{\mathbb{E}[ \epsilon^2 ]} \sqrt{\frac{s \log (p \vee n) }{n} },
\]</span> where <span class="math inline">\(s\)</span> denotes the effective dimension. Moreover, the number of regressors selected by Lasso is bounded by <span class="math display">\[\mathrm{const} \cdot s\]</span> with probability approaching <span class="math inline">\(1-\alpha\)</span> as <span class="math inline">\(n \to \infty\)</span>.</p>
</div>
</section>
<section class="slide level2">

<ul>
<li><p>Therefore, if <span class="math inline">\(s \log (p \vee n) /n\)</span> is small, Lasso and Post-Lasso regression come close to the population regression function/best linear predictor.</p></li>
<li><p>Relative to our conjectured rate <span class="math inline">\(\sqrt{s/n}\)</span>, there is an additional factor <span class="math inline">\(\sqrt{ \log (p \vee n)}\)</span> in the bound.</p></li>
<li><p>This factor captures the price of not knowing <em>a priori</em> which of the <span class="math inline">\(p\)</span> regressors are the <span class="math inline">\(s\)</span> important ones.</p></li>
<li><p>Lasso approximately finds these important predictors, but correspondingly suffers a loss relative to a predictor estimated with knowledge of the best <span class="math inline">\(s\)</span>-dimensional model.</p></li>
<li><p>A theoretical guarantee similar to <a href="#/thm-lasso" class="quarto-xref">Theorem&nbsp;1</a> has been established for cross-validated Lasso (<span class="citation" data-cites="lasso:cv">Chetverikov, Liao, and Chernozhukov (<a href="#/bibliography" role="doc-biblioref" onclick="">2021</a>)</span>).</p></li>
<li><p>Under approximate sparsity and with appropriate choice of penalty parameters, Lasso and Post-Lasso will approximate the best linear predictor well.</p></li>
</ul>
</section>
<section id="other-penalized-regression-methods-for-prediction" class="slide level2">
<h2>Other Penalized Regression Methods for Prediction</h2>
<ul>
<li><p>Instead of the Lasso penalty term, other penalty schemes can be used, leading to different regression estimators with different properties.</p></li>
<li><p>These estimators are motivated by different structures for the coefficients on the set of regressors in a high-dimensional model.</p></li>
<li><p>We consider three important settings: sparse, dense, and sparse+dense. On the next slide, <a href="#/fig-sparsedense" class="quarto-xref">Figure&nbsp;7</a> illustrates each setting.</p></li>
<li><p>We have already outlined Lasso regression, which performs best in an approximately sparse setting.</p></li>
</ul>
</section>
<section class="slide level2">


<img data-src="sparse-dense.png" class="r-stretch quarto-figure-center"><p class="caption">
Figure&nbsp;7: The Lasso penalty is best suited for approximately sparse models, and the Ridge penalty for models with small dense coefficients. The Elastic Net can be tuned to perform well with either sparse or dense coefficients. The Lava penalty is best suited for models with coefficients generated as the sum of approximately sparse coefficients and small dense coefficients.
</p></section>
<section id="ridge" class="slide level2">
<h2>Ridge</h2>
<ul>
<li><p>Next we consider the Ridge method, which performs best in the dense setting.</p></li>
<li><p>The Ridge method estimates coefficients by penalized least squares, where we minimize the sum of squared prediction error plus the penalty term given by the sum of the squared values of the coefficients times a penalty level <span class="math inline">\(\lambda\)</span>:</p></li>
</ul>
<p><span id="eq-ridge"><span class="math display">\[\hat{\beta}(\lambda)=\arg \min_{b \in \mathbb{R}^p} \sum_{i=1}^n (Y_i - b'X_{i})^2 + \lambda \sum_j b_j^2. \qquad(3)\]</span></span></p>
<ul>
<li><p>Ridge balances the complexity of the model measured by the sum of squared coefficients with the goodness of in-sample fit.</p></li>
<li><p>In contrast to Lasso, Ridge penalizes the large values of coefficients much more aggressively and small values much less aggressively – indeed, squaring big values makes them even bigger and squaring small numbers makes them even smaller.</p></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><p>Because of the latter property, Ridge does not set estimated coefficients to zero and so it does not do variable selection.</p></li>
<li><p>The Ridge predictor <span class="math inline">\(\hat \beta'X\)</span> is especially well suited for prediction in “dense” models, where the <span class="math inline">\(\beta_j\)</span>’s are all small without necessarily being approximately sparse.</p></li>
<li><p>In the dense case, the Ridge predictor can easily outperform the Lasso predictor.</p></li>
<li><p>Finally, we note that, in practice, we can choose the penalty level <span class="math inline">\(\lambda\)</span> in Ridge by cross-validation (or sample splitting).</p></li>
</ul>
</section>
<section id="elastic-net" class="slide level2">
<h2>Elastic Net</h2>
<ul>
<li><p>Ridge and Lasso have other useful modifications or hybrids that can perform well in the sparse, dense or sparse + dense settings.</p></li>
<li><p>One popular modification is the Elastic Net (<span class="citation" data-cites="elnet">Zou and Hastie (<a href="#/bibliography" role="doc-biblioref" onclick="">2005</a>)</span>) that can perform well in either the sparse or the dense scenario with appropriate tuning (though not in the sparse+dense case).</p></li>
<li><p>The Elastic Net method estimates coefficients by penalized least squares with the penalty given by a linear combination of the Lasso and Ridge penalties: <span id="eq-enet"><span class="math display">\[\hat{\beta}(\lambda_1, \lambda_2)=\arg \min_{b \in \mathbb{R}^p} \sum_i (Y_i - b'X_{i})^2 + \lambda_1  \sum_j b_j^2  + \lambda_2 \sum_j |b_j|. \qquad(4)\]</span></span></p></li>
<li><p>We see that the penalty function has two penalty levels <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span>, which could be chosen by cross-validation in practice.</p></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><p>By selecting different values of penalty levels <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span>, we have more flexibility with Elastic Net for building a good prediction rule than with just Ridge or Lasso.</p></li>
<li><p>The Elastic Net performs variable selection unless we completely shut down the Lasso penalty by setting <span class="math inline">\(\lambda_2 =0\)</span>.</p></li>
<li><p>With proper tuning, Elastic Net works well in regression models where regression coefficients are either approximately sparse or dense.</p></li>
<li><p>We don’t yet have good theoretical guarantees on predictive performance for the Elastic Net method.</p></li>
</ul>
</section>
<section id="choice-of-regression-methods-in-practice" class="slide level2">
<h2>Choice of Regression Methods in Practice</h2>
<ul>
<li><p>How should we select the appropriate penalized regression method?</p></li>
<li><p>The answer is simple if we are interested in building the best prediction. We can use data splitting into training and testing sets, and simply choose the method that performs the best on the test set.</p></li>
<li><p>We show an example of this approach in the R notebook on predicting wages in CPS 2015 data. We can also use ensemble methods to aggregate prediction methods to get boosts in predictive performance.</p></li>
</ul>
</section>
<section id="introduction-to-non-linear-regression-methods" class="slide level2">
<h2>Introduction to Non-linear Regression Methods</h2>
<ul>
<li><p>Here we discuss nonlinear regression methods based on tree models.</p></li>
<li><p>Tree-based methods include regression trees, random forests, and boosted trees. Regression trees are great for exploration and explainable analytics, while random forests and boosted trees are great predictive tools for structured data and data sets of intermediate size (say, up to several million observations).</p></li>
<li><p>We are interested in predicting an outcome <span class="math inline">\(Y\)</span> using raw regressors <span class="math inline">\(Z\)</span>, which are <span class="math inline">\(k\)</span>-dimensional. The best prediction rule <span class="math inline">\(g(Z)\)</span> under square loss is the conditional expectation (CE) of <span class="math inline">\(Y\)</span> given <span class="math inline">\(Z\)</span>: <span class="math display">\[
g(Z) = \mathrm{E}(Y |Z).
\]</span></p></li>
<li><p>In previous chapters, we used best linear prediction rules to approximate <span class="math inline">\(g(Z)\)</span> and linear regression or Lasso regression for estimation.</p></li>
<li><p>Now we consider nonlinear prediction rules to approximate <span class="math inline">\(g(Z)\)</span>, focusing on tree-based methods.</p></li>
</ul>
</section>
<section id="important" class="slide level2">
<h2>Important!</h2>
<ul>
<li><p>The use of Best Prediction rules (CEs) is not just important for generating good predictions, but is crucial for causal inference.</p></li>
<li><p>Identification of causal parameters such as ATE via conditioning strategies requires us to work with CEs rather than with best linear prediction rules.</p></li>
<li><p>Previously we tried to make best linear prediction rules flexible to try to approximate best prediction rules. In the following, we explore fully nonlinear strategies.</p></li>
</ul>
</section>
<section id="introduction-to-regression-trees" class="slide level2">
<h2>Introduction to Regression Trees</h2>
<ul>
<li><p>Regression Trees are based on partitioning the regressor space (the space where <span class="math inline">\(Z\)</span> takes on values) into a set of rectangles. A simple model is then fit within each rectangle.</p></li>
<li><p>The most common approach fits a simple constant model within each rectangle, which corresponds to approximating the unknown function by a “step function”. Given a partition into <span class="math inline">\(M\)</span> regions, denoted <span class="math inline">\(R_1, \ldots, R_M\)</span> the approximating function when a constant is fit within each rectangle is given by <span class="math display">\[ f(z)=\sum_{m=1}^M \beta_m 1(z\in R_m),\]</span> where <span class="math inline">\(\beta_m, m=1,\ldots,M,\)</span> denotes a constant for each region and <span class="math inline">\(1(\cdot)\)</span> denotes the indicator function.</p></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><p>Suppose we have <span class="math inline">\(n\)</span> observations <span class="math inline">\((Z_i,Y_i)\)</span> for <span class="math inline">\(i=1,\ldots,n.\)</span> The estimated coefficients for a given partition are obtained by minimizing the in-sample MSE: <span class="math display">\[
\hat \beta = \arg\min_{b_1,..., b_M} \mathbb{E}_n\left (Y_i -  \sum_{m=1}^M b_m 1(Z_i \in R_m) \right )^2,
\]</span> so that <span class="math display">\[ \hat \beta_m = \text{ average of } Y_i \text{ where } Z_i \in R_m.\]</span></p></li>
<li><p>The regions <span class="math inline">\(R_1, \ldots, R_M\)</span> are called nodes, and each node <span class="math inline">\(R_m\)</span> has a predicted value <span class="math inline">\(\hat \beta_m\)</span> associated with it.</p></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><a href="#/fig-tree1" class="quarto-xref">Figure&nbsp;8</a> illustrates a simple regression tree for the wage data:</li>
</ul>

<img data-src="reg-tree-1-1.png" class="r-stretch quarto-figure-center"><p class="caption">
Figure&nbsp;8: The bottom nodes on the tree provide prediction rules for different subsets of observations. For example, the predicted hourly wage for a college educated worker with 9.5 or more years of experience (a worker with college = 1 and exper <span class="math inline">\(\geq\)</span> 9.5) is 24 dollars.
</p></section>
<section class="slide level2">

<ul>
<li><p>This tree has a depth of two, meaning that predictions are produced as a sequence of two binary decisions (or partitions of the data).</p></li>
<li><p>Starting at the top of the tree and working down provides a simple prediction rule for any observation.</p></li>
<li><p>The key feature of trees is that the cut points for the partitions are adaptively chosen based on the data. That is, the splits are not pre-specified but are purely data dependent. So, how did we use the data to grow the tree in <a href="#/fig-tree1" class="quarto-xref">Figure&nbsp;8</a> ?</p></li>
<li><p>To make computation tractable, we use recursive binary partitioning or splitting of the regressor space. First, we cut the regressor space into two regions by choosing the regressor and splitting point such that using the prediction rule fit within each region produces the best improvement in the in-sample MSE.</p></li>
<li><p>Finding this split point requires trying the partition produced by splitting the data along every possible value of every observed variable. That is, we are neither pre-specifying which variables nor which split points are important in providing a good prediction rule.</p></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>Applying this procedure in the wage data gives us the depth 1 tree shown in <a href="#/fig-tree2" class="quarto-xref">Figure&nbsp;9</a>. In this case, the best regressor to split on is the indicator of college degree, that takes values 0 or 1. Here splitting at any point between 0 and 1 provides the same rule, and an often used convention for binary variables is to use the “natural” split point of 0.5. Applying this split point yields the initial prediction rule: an hourly wage of <span class="math inline">\(20\)</span> for college graduates and <span class="math inline">\(13\)</span> for others.</li>
</ul>

<img data-src="reg-tree-2-1.png" class="r-stretch quarto-figure-center"><p class="caption">
Figure&nbsp;9: Depth 1 tree in the wage example
</p></section>
<section class="slide level2">

<ul>
<li><p>To grow the tree to depth 2, we then repeat the procedure for choosing the first partition rule within the two regions resulting from the first step. This step will result in a partition of the covariate space into four new regions.</p></li>
<li><p>It is important to note that the two splits produced at this point may use different variables/splitting points than before.</p></li>
<li><p>This feature means that the tree alogirthm can create “interactions” and “nonlinearities” without requiring input from the user.</p></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>To grow deeper trees corresponding to more complex prediction rules, we simply keep repeating. We stop when the desired depth of the tree is reached, or when a prespecified minimal number of observations per region, called minimal node size, is reached.</li>
</ul>

<img data-src="reg-tree-4-1.png" class="r-stretch quarto-figure-center"><p class="caption">
Figure&nbsp;10: Depth 3 tree in the wage example. The depth of three was chosen to avoid getting headaches from looking at a more complicated tree.
</p></section>
<section class="slide level2">

<ul>
<li><p>First, the deeper we grow the tree, the better is our approximation to the regression function <span class="math inline">\(g(Z)\)</span>.</p></li>
<li><p>However, the deeper the tree, the noisier our estimate <span class="math inline">\(\hat g(Z)\)</span> becomes (overfitting), since there are fewer observations per terminal node to estimate the predicted value for this node.</p></li>
<li><p>From a prediction point of view, we can try to find the right depth or the structure of the tree by sample-splitting (cross-validation). For example, in the wage example, the tree of depth 2 performs better in terms of cross-validated MSE than the tree of depth 3 or 1.</p></li>
</ul>
</section>
<section id="random-forest" class="slide level2">
<h2>Random Forest</h2>
<ul>
<li><p>In practice, regression trees often do not provide the best predictive performance, because a single regression tree provides a relatively crude approximation to a smooth regression function <span class="math inline">\(g(Z)\)</span>.</p></li>
<li><p>We illustrate the potential poor approximation of regression trees in <a href="#/fig-shallow" class="quarto-xref">Figure&nbsp;11</a> and <a href="#/fig-deep" class="quarto-xref">Figure&nbsp;12</a>. These figures simply illustrate that step functions, which are the outputs of typical regression tree implementations, struggle in approximating smooth functions.</p></li>
<li><p>A powerful and widely used approach that aims to improve upon simple regression trees is to build a <em>Random Forest,</em> as proposed by Leo Breiman (<span class="citation" data-cites="breiman">Breiman (<a href="#/bibliography" role="doc-biblioref" onclick="">2001</a>)</span>).</p></li>
<li><p>The idea of a Random Forest is to grow many different deep trees that have low approximation error and then average the prediction rules across trees.</p></li>
</ul>
</section>
<section class="slide level2">


<img data-src="TreeApprox.png" class="r-stretch quarto-figure-center"><p class="caption">
Figure&nbsp;11: Approximation of <span class="math inline">\(g(Z) = \exp(4 Z)\)</span> by a shallow Regression Tree in the noiseless case.
</p></section>
<section class="slide level2">


<img data-src="DeepTreeaApprox.png" class="r-stretch quarto-figure-center"><p class="caption">
Figure&nbsp;12: Approximation of <span class="math inline">\(g(Z) = \exp(4 Z)\)</span> by a deep Regression Tree in the noiseless case.
</p></section>
<section id="bagging" class="slide level2">
<h2>Bagging</h2>
<ul>
<li><p>To produce different trees using only the observed data, the trees going into a random forest are grown from artificial data generated by sampling randomly with replacement from the original data; that is, each tree in a random forest is fit to a bootstrap sample.</p></li>
<li><p>Within the bootstrap samples, trees are grown deep to keep approximation error low.</p></li>
<li><p>Averaging across the trees produced in the bootstrap samples is then meant to reduce the noisiness of the individual trees.</p></li>
<li><p>The procedure of averaging noisy prediction rules over bootstrap samples is called Bootstrap Aggregation or <em>Bagging</em>.</p></li>
<li><p>When the data set is large, we can also rely on fitting trees within subsamples instead of using the bootstrap. Using subsamples offers some computational advantages and also simplifies theoretical analysis.</p></li>
</ul>
</section>
<section id="bootstrap-samples" class="slide level2">
<h2>Bootstrap Samples</h2>
<ul>
<li><p>Each bootstrap sample is created by sampling from our data on pairs <span class="math inline">\((Y_i,Z_i)\)</span> randomly, with replacement.</p></li>
<li><p>Hence, some observations are drawn multiple times and some aren’t redrawn at all.</p></li>
<li><p>Given a bootstrap sample, indexed by <span class="math inline">\(b\)</span>, we build a tree-based prediction rule <span class="math inline">\(\hat g_b(Z)\)</span>.</p></li>
<li><p>We repeat the procedure <span class="math inline">\(B\)</span> times in total, and then average the prediction rules that result from each of the bootstrap samples: <span class="math display">\[
\hat g_{\mathrm{random \ forest}} (Z) = \frac{1}{B} \sum_{b=1}^B \hat g_b(Z).
\]</span></p></li>
</ul>
</section>
<section class="slide level2">


<img data-src="RFApprox.png" class="r-stretch quarto-figure-center"><p class="caption">
Figure&nbsp;13: Approximation of <span class="math inline">\(g(Z) = \exp(4 Z)\)</span> by a Random Forest in the noiseless case.
</p></section>
<section class="slide level2">

<ul>
<li><p>If we could have many independent copies of the data, we could obtain low-bias but potentially very noisy prediction rules in each copy of the data and then average the prediction rules obtained over these copies to reduce the noise.</p></li>
<li><p>Since we don’t have many copies in reality, we rely on the bootstrap to create many quasi-copies of the data. Another feature of this idea is that the cut-points defining partitions for the tree obtained within each bootstrap sample will be different, producing a different step function approximation.</p></li>
<li><p>Averaging over many step functions with steps at different locations will potentially produce a much smoother approximation to the underlying function. The improved approximation relative to simple trees is illustrated in <a href="#/fig-rf" class="quarto-xref">Figure&nbsp;13</a>.</p></li>
<li><p>The most important modification of the simple version of bootstrap aggregation is the use of additional randomization to “decorrelate” the trees: When we build trees over different bootstrap samples, we also randomize over the variables that trees are allowed to use in forming partitions. This additional layer of randomization results in trees having different structure.</p></li>
</ul>
</section>
<section id="boosted-trees" class="slide level2">
<h2>Boosted Trees</h2>
<ul>
<li><p>The idea of boosting is that of a recursive fitting: We estimate a simple prediction rule, then take the residuals and estimate another simple prediction rule for these residuals, and so on. A sum of the prediction rules for the residuals then gives us the prediction rule for the outcome.</p></li>
<li><p>A common use of boosting is with regression trees. Here we use shallow trees as the simple prediction rule. Shallow trees produce low noise prediction rules, but also tend to have high approximation error.</p></li>
<li><p>However, each step where a model is fit to the residuals from the previous step reduces the approximation error. In order to avoid overfitting, we can stop the procedure once there is no marginal improvement to the cross-validated MSE. T</p></li>
<li><p>The improved approximation of boosted trees relative to simple trees is illustrated in the following <a href="#/fig-bt" class="quarto-xref">Figure&nbsp;14</a>.</p></li>
</ul>
</section>
<section class="slide level2">


<img data-src="BTApprox.png" class="r-stretch quarto-figure-center"><p class="caption">
Figure&nbsp;14: Approximation of <span class="math inline">\(g(Z) = \exp(4 Z)\)</span> by Boosted Trees in the noiseless case with a sufficient number of steps <span class="math inline">\(J\)</span>.
</p></section>
<section id="the-boosting-algorithm" class="slide level2">
<h2>The Boosting Algorithm</h2>
<ol type="1">
<li><p>Initialize the residuals: <span class="math inline">\(R_i := Y_i, i=1,...,n\)</span>.</p></li>
<li><p>For <span class="math inline">\(j=1,...,J\)</span>, fit a tree-based prediction rule <span class="math inline">\(\hat g_j(Z)\)</span> to the data <span class="math inline">\((Z_i, R_i)_{i=1}^n\)</span> and update the residuals <span class="math inline">\(R_i := R_i - \lambda \hat g_j(Z_i)\)</span>, where <span class="math inline">\(\lambda\)</span> is called the learning rate.</p></li>
<li><p>Output the boosted prediction rule: <span class="math display">\[
\hat g(Z) := \sum_{j=1}^J  \lambda \hat g_j(Z).
\]</span></p></li>
</ol>
</section>
<section class="slide level2">

<ul>
<li><p>In practice, using boosted trees requires making several choices. One needs to define the tree-based prediction rule used at each step and also choose the number of learning steps, <span class="math inline">\(J\)</span>, and the learning rate, <span class="math inline">\(\lambda\)</span>.</p></li>
<li><p>These tuning parameters can be chosen by cross-validation. A default value for <span class="math inline">\(\lambda\)</span> is <span class="math inline">\(0.1\)</span>, <span class="math inline">\(0&lt;\lambda&lt;1\)</span>.</p></li>
<li><p>The idea is to fit simple prediction rules, so one will typically specify the prediction rule by setting the depth of the trees to a small number. For example, at each step, the prediction rule may be a regression tree of depth two.</p></li>
<li><p>Note that the boosting algorithm is quite general and can be applied to non-tree uses, see, e.g., <span class="citation" data-cites="KUECK2023714">Kueck et al. (<a href="#/bibliography" role="doc-biblioref" onclick="">2023</a>)</span> for so-called <span class="math inline">\(L_2\)</span>-boosting using ols as a simple prediction rule.</p></li>
<li><p>A very popular implementation widely used in industry is <em>xgboost</em>, which has the capability to impose qualitative shape constraints like monotonicity in one or several variables.</p></li>
</ul>
</section>
<section id="prediction-quality-of-modern-nonlinear-regression-methods" class="slide level2">
<h2>Prediction Quality of Modern Nonlinear Regression Methods</h2>
<ul>
<li><p>The best prediction rule for an outcome <span class="math inline">\(Y\)</span> using features/regressors <span class="math inline">\(Z\)</span> is the function <span class="math inline">\(g(Z)\)</span>, equal to the conditional expectation of <span class="math inline">\(Y\)</span> using <span class="math inline">\(Z\)</span>: <span class="math display">\[
g(Z) = \mathbb{E}[Y \mid Z].
\]</span></p></li>
<li><p>Theoretical work demonstrates that under appropriate regularity conditions and with appropriate choices of tuning parameters, the mean squared approximation error of prediction rules produced by modern nonlinear regression methods is small once the sample size <span class="math inline">\(n\)</span> is sufficiently large, namely, <span class="math display">\[
\| \hat g- g\|_{L^2(Z)}  = \sqrt{\mathbb{E}_Z[( \hat g(Z) - g(Z))^2 ]} \to 0, \quad \text{ as } n \to \infty,
\]</span> where <span class="math inline">\(\mathbb{E}_Z\)</span> denotes the expectation taken over <span class="math inline">\(Z\)</span>, holding everything else fixed (see e.g in <span class="citation" data-cites="wager:athey">Wager and Athey (<a href="#/bibliography" role="doc-biblioref" onclick="">2018</a>)</span> and <span class="citation" data-cites="syrgkanis:2020">Syrgkanis and Zampetakis (<a href="#/bibliography" role="doc-biblioref" onclick="">2020</a>)</span>).</p></li>
</ul>
</section>
<section id="when-do-neural-nets-win" class="slide level2">
<h2>When do neural nets win?</h2>
<ul>
<li><p>In a recent example, <span class="citation" data-cites="Bajari-hedonic">Bajari et al. (<a href="#/bibliography" role="doc-biblioref" onclick="">2021</a>)</span> are interested in predicting prices of products given their characteristics, which include both text and images.</p></li>
<li><p>In this example, neural networks (specifically BERT and ResNet50) are first used to convert the text and image data into several thousand-dimensional numerical features <span class="math inline">\(X\)</span> (called embeddings).</p></li>
<li><p>These features extracted from the text and image data are then used as input variables in a deep neural network for predicting product prices. The deep neural network used in the example consists of 3 hidden layers, with the penultimate layer consisting of about 400 neurons.</p></li>
<li><p>The data set used in this example is larger than <strong>10 million observations</strong>. The accuracy of prediction for the deep neural network described above, as measured by the <span class="math inline">\(R^2\)</span> on the test sample, is about <span class="math inline">\(90\%\)</span>.</p></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><p>In contrast, random forests applied to predict prices using the text and image embeddings as inputs deliver an <span class="math inline">\(R^2\)</span> in the test sample that is in the ballpark of <span class="math inline">\(80\%\)</span>, and a linear model estimated via least squares that uses the text and image embeddings as predictor variables delivers an <span class="math inline">\(R^2\)</span> in the test sample of only around <span class="math inline">\(70\%\)</span>.</p></li>
<li><p>Ignoring the neural network embeddings of the text and image data and using only simple catalog features, the <span class="math inline">\(R^2\)</span> is lower than <span class="math inline">\(40\%\)</span>.</p></li>
</ul>
</section>
<section id="trust-but-verify" class="slide level2">
<h2>Trust but Verify</h2>
<ul>
<li><p>Both tree-based methods and neural networks provide powerful, flexible models that can deliver high-quality approximations of regression functions. However, the high degree of flexibility can lead to overfitting. Therefore, it is always important to verify the performance on test data to make sure that the predictive model being used is actually a good one.</p></li>
<li><p>A simple verification procedure is data splitting, which can be performed in the following way:</p></li>
</ul>
<ol type="1">
<li><p>We use a random subset of data for estimating/training the prediction rule.</p></li>
<li><p>We use the other part of the data to evaluate the quality of the prediction rule, recording out-of-sample mean squared error, <span class="math inline">\(R^2\)</span>, or some other desired measure of prediction quality.</p></li>
</ol>
<ul>
<li>Recall that the part of the data used for estimation is called the training sample. The part of the data used for evaluation is called the testing or validation/test sample.</li>
</ul>
</section>
<section id="combining-predictions---ensemble-learning" class="slide level2">
<h2>Combining Predictions - Ensemble Learning</h2>
<ul>
<li><p>Given different prediction rules, we can choose either a single method or an aggregation of several methods as our prediction approach. An aggregated prediction (“ensemble”) is a linear combination of the basic predictors.</p></li>
<li><p>Specifically, we consider an aggregated prediction rule of the form: <span class="math display">\[\tilde g(Z) = \sum_{k=1}^K  \tilde \alpha_k \hat g_k(Z),\]</span> where <span class="math inline">\(\hat g_k\)</span>’s denote basic predictors, potentially including a constant. The basic predictors are computed on the training data.</p></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><p>If the number of prediction rules, <span class="math inline">\(K\)</span>, is small, we can figure out the coefficients of the optimal linear combination of the rules, <span class="math inline">\(\tilde \alpha_k\)</span>, using test data <span class="math inline">\(V\)</span> by simply running least squares of the outcomes in the test data on their associated predicted values: <span class="math display">\[
\min_{(\alpha_k)_{k=1}^K} \sum_{i \in V}  (Y_i - \sum_{k=1}^K \alpha_k \hat{g}_k(Z_i))^2.
\]</span></p></li>
<li><p>Here we are minimizing the sum of squared prediction errors in the test sample using the prediction rules from the training sample as the regressors. If we wish to evaluate the predictive performance, we need a third validation sample.</p></li>
<li><p>If <span class="math inline">\(K\)</span> is large, we can instead use Lasso for aggregation:</p></li>
</ul>
<p><span class="math display">\[
  \min_{(\alpha_k)_{k=1}^K} \sum_{i \in V}  (Y_i - \sum_{k=1}^K \alpha_k \hat{g}_k(Z_i))^2 + \lambda \sum_{k=1}^K | \alpha_k|.
\]</span></p>
</section>
<section id="auto-ml-frameworks" class="slide level2">
<h2>Auto ML Frameworks</h2>
<ul>
<li>There are a variety of new frameworks emerging that do automated search and aggregation of different prediction methods. These automatic aggregation procedures use approaches like the one we outlined above or other heuristics. Examples of automatic aggregation methods include H20, AutoML (<span class="citation" data-cites="ledell2020h2o">LeDell and Poirier (<a href="#/bibliography" role="doc-biblioref" onclick="">2020</a>)</span>), and Auto Gluon (<span class="citation" data-cites="erickson2020autogluon">Erickson et al. (<a href="#/bibliography" role="doc-biblioref" onclick="">2020</a>)</span>), which relies on Neural Nets.</li>
</ul>
</section>
<section id="cross-validation" class="slide level2">
<h2>Cross-Validation</h2>
<ul>
<li>Cross-validation is a common practical tool that provides a way to choose tuning parameters such as the penalty level. The idea of cross-validation is to rely on repeated splitting of the training data to estimate the potential out-of-sample predictive performance.</li>
</ul>
<p><strong>Cross-Validation in Words:</strong></p>
<ol type="1">
<li><p>We partition the data into <span class="math inline">\(K\)</span> blocks called “folds”, for example, with <span class="math inline">\(K=5\)</span>, we split the data into 5 non-overlapping blocks.</p></li>
<li><p>Leave one block out. Fit a prediction rule on all the other blocks. Predict the outcome observations in the left out block, and record the empirical Mean Squared Prediction Error. Repeat this for each block.</p></li>
<li><p>Average the empirical Mean Squared Prediction Errors over blocks.</p></li>
</ol>
<p>We do these steps for several or many values of the tuning parameters and choose the value of the tuning parameter that minimizes the Averaged Mean Squared Prediction Error.</p>
</section>
<section id="cross-validation-formal-description" class="slide level2">
<h2>Cross-Validation: Formal Description</h2>
<ol type="1">
<li><p>Randomly select a partition of observation indices <span class="math inline">\(1,...., n\)</span> in <span class="math inline">\(K\)</span> random folds <span class="math inline">\(B_1,..., B_K\)</span>.</p></li>
<li><p>For each <span class="math inline">\(k =1,...,K\)</span>, fit a prediction rule denoted by <span class="math inline">\(\hat f^{[-k]}(\cdot; \theta)\)</span>, where <span class="math inline">\(\theta\)</span> denotes the tuning parameters and <span class="math inline">\(\hat f^{[-k]}\)</span> depends only on observations <strong>not</strong> in the fold <span class="math inline">\(B_k\)</span>.</p></li>
</ol>
<p>3.For each <span class="math inline">\(k=1,...,K\)</span>, the empirical out-of-sample MSE for the block <span class="math inline">\(B_k\)</span> is <span class="math display">\[
  \text{MSE}_k(\theta) = \frac{1}{m_k} \sum_{i \in B_k} (Y_i -  \hat f^{[-k]}(X_i; \theta))^2,
\]</span> where <span class="math inline">\(m_k\)</span> is the size of the block <span class="math inline">\(B_k\)</span>.</p>
<ol start="4" type="1">
<li>Compute the cross-validated MSE as <span class="math inline">\(\text{CV-MSE}(\theta) = \frac{1}{K} \sum_{k=1}^K \text{MSE}_k (\theta)\)</span>. Choose the tuning parameter <span class="math inline">\(\hat \theta\)</span> as a minimizer of <span class="math inline">\(\text{CV-MSE}(\theta)\)</span>.</li>
</ol>
</section>
<section id="notebooks" class="slide level2">
<h2>Notebooks</h2>
<ul>
<li><p><a href="https://drive.google.com/file/d/1Q-Y8kCxAgNc7_OMIPeP1Al1tIf4EKJaf/view?usp=sharing">R Notebook on Penalized Regressions</a> provides details of implementation of different penalized regression methods and examines their performance for approximating regression functions in a simulation experiment.</p></li>
<li><p><a href="https://drive.google.com/file/d/1JZvv_jRcivKMa8wtH1kpa73rxSEiezn6/view?usp=sharing">R Notebook on ML-based Prediction of Wages</a> provides details of implementation of penalized regression, regression trees, random forest and boosted tree methods, a comparison of various methods and a way to choose the best method or create an ensemble of methods.</p></li>
<li><p><a href="https://drive.google.com/file/d/19BXAPM6a6SHqmFuxTo18dDb2fW6Oto5x/view?usp=sharing">R Notebook on AutoML Prediction of Wages</a> provides an application of the H20 AutoML framework to the wage prediction problem.</p></li>
</ul>
</section>
<section id="bibliography" class="slide level2 smaller scrollable">
<h2>Bibliography</h2>
<div class="quarto-auto-generated-content">
<div class="footer footer-default">
<p>CML</p>
</div>
</div>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Bajari-hedonic" class="csl-entry" role="listitem">
Bajari, Patrick L., Zhihao Cen, Victor Chernozhukov, Manoj Manukonda, Jin Wang, Ramon Huerta, Junbo Li, et al. 2021. <span>“Hedonic Prices and Quality Adjusted Price Indices Powered by AI.”</span> cemmap working paper.
</div>
<div id="ref-BC-PostLASSO" class="csl-entry" role="listitem">
Belloni, Alexandre, and Victor Chernozhukov. 2013. <span>“Least Squares After Model Selection in High-Dimensional Sparse Models.”</span> <em>Bernoulli</em> 19 (2): 521–47.
</div>
<div id="ref-BickelRitovTsybakov2009" class="csl-entry" role="listitem">
Bickel, Peter J., Ya’acov Ritov, and Alexandre B. Tsybakov. 2009. <span>“Simultaneous Analysis of <span>L</span>asso and <span>D</span>antzig Selector.”</span> <em>Annals of Statistics</em> 37 (4): 1705–32.
</div>
<div id="ref-breiman" class="csl-entry" role="listitem">
Breiman, Leo. 2001. <span>“Random Forests.”</span> <em>Machine Learning</em> 45 (1): 5–32.
</div>
<div id="ref-lasso:cv" class="csl-entry" role="listitem">
Chetverikov, Denis, Zhipeng Liao, and Victor Chernozhukov. 2021. <span>“On Cross-Validated Lasso in High Dimensions.”</span> <em>Annals of Statistics</em> 49 (3): 1300–1317.
</div>
<div id="ref-erickson2020autogluon" class="csl-entry" role="listitem">
Erickson, Nick, Jonas Mueller, Alexander Shirkov, Hang Zhang, Pedro Larroy, Mu Li, and Alexander Smola. 2020. <span>“Autogluon-Tabular: Robust and Accurate Automl for Structured Data.”</span> <em>arXiv Preprint arXiv:2003.06505</em>.
</div>
<div id="ref-KUECK2023714" class="csl-entry" role="listitem">
Kueck, Jannis, Ye Luo, Martin Spindler, and Zigan Wang. 2023. <span>“Estimation and Inference of Treatment Effects with L2-Boosting in High-Dimensional Settings.”</span> <em>Journal of Econometrics</em> 234 (2): 714–31. https://doi.org/<a href="https://doi.org/10.1016/j.jeconom.2022.02.005">https://doi.org/10.1016/j.jeconom.2022.02.005</a>.
</div>
<div id="ref-ledell2020h2o" class="csl-entry" role="listitem">
LeDell, Erin, and Sebastien Poirier. 2020. <span>“H2o Automl: Scalable Automatic Machine Learning.”</span> In <em>Proceedings of the AutoML Workshop at ICML</em>. Vol. 2020.
</div>
<div id="ref-syrgkanis:2020" class="csl-entry" role="listitem">
Syrgkanis, Vasilis, and Manolis Zampetakis. 2020. <span>“Estimation and Inference with Trees and Forests in High Dimensions.”</span> In <em>Proceedings of Thirty Third Conference on Learning Theory</em>, edited by Jacob Abernethy and Shivani Agarwal, 125:3453–54. Proceedings of Machine Learning Research. PMLR.
</div>
<div id="ref-wager:athey" class="csl-entry" role="listitem">
Wager, Stefan, and Susan Athey. 2018. <span>“Estimation and Inference of Heterogeneous Treatment Effects Using Random Forests.”</span> <em>Journal of the American Statistical Association</em> 113 (523): 1228–42.
</div>
<div id="ref-elnet" class="csl-entry" role="listitem">
Zou, Hui, and Trevor Hastie. 2005. <span>“Regularization and Variable Selection via the Elastic Net.”</span> <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 67 (2): 301–20.
</div>
</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="L3_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="L3_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="L3_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="L3_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="L3_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="L3_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="L3_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="L3_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="L3_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="L3_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="L3_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true,"boardmarkerWidth":5,"theme":"chalkboard"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        // For code content inside modals, clipBoardJS needs to be initialized with a container option
        // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>